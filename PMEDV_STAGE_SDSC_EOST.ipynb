{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa0c478f",
   "metadata": {},
   "source": [
    "# Auteur : PAMBOU MOUBOGHA Eddy Vianney\n",
    "# Etudes : Master Sciences des Donnees et Systèmes Complexes\n",
    "# Ecole : Université de Strasbourg\n",
    "# Sujet :  Detection de glissements de terrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "1b2219c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from datetime import date, datetime\n",
    "import datetime\n",
    "from tslearn.barycenters import dtw_barycenter_averaging\n",
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA, FastICA\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.feature_selection import f_regression\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from sklearn.manifold import TSNE\n",
    "import hdbscan\n",
    "import csv\n",
    "import multiprocessing\n",
    "import itertools\n",
    "import operator\n",
    "import math\n",
    "from dtaidistance import dtw\n",
    "import seaborn as sns\n",
    "import elevation\n",
    "import json\n",
    "plt.style.use('fivethirtyeight')\n",
    "from osgeo import gdal \n",
    "from subprocess import Popen\n",
    "import simplekml\n",
    "import copy\n",
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378b759e",
   "metadata": {},
   "source": [
    "# Attributs des fichiers MM_TIO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19355f89",
   "metadata": {},
   "source": [
    "* Lat : latitude (degré)\n",
    "* Lon : longitude (degré)\n",
    "* Vel : vitesse (mètre/jour)\n",
    "* Topo : altitude d'un point (mètre)\n",
    "* TS : serie temporelles de déplacement (mètre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "212656fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = './donnees'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f392345",
   "metadata": {},
   "source": [
    "# Paramètres des géométries ascendante et descendante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d07c2e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# angle d'incidence du satellite en géométrie ascendante (angle entre la vertical et la direction du  satellite)\n",
    "theta_asc = 0\n",
    "# angle d'incidence du satelitte en géométrie descendante (angle entre la vertical et la direction du satellite)\n",
    "theta_desc = 0\n",
    "# difference angulaire des orbites des géométries ascendante et descendante\n",
    "delta_alpha =  0\n",
    "# déplacement le long de la ligne de visée en géométrie ascendante\n",
    "d_los_asc = 0\n",
    "# déplacement le long de la ligne de visée en géométrie descendante\n",
    "d_los_dsc = 0\n",
    "# # déplacement vertical\n",
    "d_up = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54df763e",
   "metadata": {},
   "source": [
    "# Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15a1c987",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_slope_value(file, lat, lon, ref='wgs84'):\n",
    "    return float(os.popen('gdallocationinfo -valonly -%s %s %f %f' % (ref, file, lat, lon)).read())\n",
    "    \n",
    "def plot_raster(raster, cmap='gray'):\n",
    "    values = raster.GetRasterBand(1).ReadAsArray()\n",
    "    plt.figure()\n",
    "    plt.imshow(values, cmap = cmap)\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6d7e54e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.899076461792\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_slope_value(ref, file, lat, lon):\n",
    "        val = os.popen('gdallocationinfo -valonly -%s %s %f %f' % (ref, file, lat, lon)).read()\n",
    "        if len(val) == 0:\n",
    "            raise ValueError('La pente est non valide !')\n",
    "        print(val)\n",
    "        return  float(val)\n",
    "    \n",
    "ref = 'wgs84'\n",
    "file = 'rasters/slope_map.tif'\n",
    "lat = 6.662360\n",
    "lon = 44.394169\n",
    "\n",
    "val = os.popen('gdallocationinfo -valonly -%s %s %f %f' % (ref, file, lat, lon)).read()\n",
    "print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "ef431d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dem = gdal.Open('rasters/31TGK_copernicus_dem.tif')\n",
    "slope_map = gdal.DEMProcessing('rasters/slope_map.tif', dem, 'slope', computeEdges = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "0623a569",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_json():\n",
    "    for i, (dirpath, dirnames, filenames) in enumerate(os.walk('./secteurs')):\n",
    "        file_path = os.path.join(dirpath, filename)\n",
    "        file = json.load(open(file_path))\n",
    "        nb_lines   = p['local']['nb_lines']\n",
    "        nb_columns = p['local']['nb_columns']\n",
    "        for nl in range(nb_lines):\n",
    "            for nc in range(nb_columns):\n",
    "                latitude  = file['data'][nl][nc][0]\n",
    "                longitude = file['data'][nl][nc][1]\n",
    "                elevation = file['data'][nl][nc][2]\n",
    "                velocity  = file['data'][nl][nc][3]\n",
    "                quality   = file['data'][nl][nc][4]\n",
    "                serie     = file['data'][nl][nc][5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "708aa22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_date(str):\n",
    "    str_strp = str.strip()\n",
    "    year, month, day = int(str_strp[0:4]), int(str_strp[4:6]), int(str_strp[6:8])\n",
    "    return date(year, month, day)\n",
    "\n",
    "def load_data(filename):\n",
    "\n",
    "    # numéro de la ligne ou commence les données\n",
    "    num_start = 44\n",
    "    # numéro de la ligne ou se trouve la liste des dates\n",
    "    num_list_dates = 40\n",
    "    # attributs présent dans les données\n",
    "    columns = ['id', 'Lat','Lon', 'Topo', 'Vel', 'Coer',' CosN', 'CosE', 'CosU']\n",
    "    # dictionnaire stockant les données\n",
    "    data = {column: [] for column in columns}\n",
    "    # liste des dates \n",
    "    indexes = []\n",
    "    # series temporelles\n",
    "    series = []\n",
    "    # liste de dataframes\n",
    "    df_series = []\n",
    "\n",
    "    with open(DATA_PATH + '/' + filename) as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=\",\")\n",
    "        line_count = 1 \n",
    "        for row in csv_reader:\n",
    "            if line_count == num_list_dates:\n",
    "                indexes = [row[0].split(' ')[1]] + row[1:]\n",
    "            if line_count >= num_start:\n",
    "                # extraction des premiers attributs\n",
    "                for i in range(len(columns)):\n",
    "                    data[columns[i]].append(row[i])\n",
    "                # extraction de l'attribut TS(série temporelle)\n",
    "                series.append([float(v) for v in row[len(columns):]])\n",
    "            line_count  += 1\n",
    "        if len(indexes) != len(series[0]):\n",
    "            print('Erreur : Les indexes et les valeurs ne correspondent pas')\n",
    "        # convertir les index en date\n",
    "        indexes = [d.strip()[0:8] for d in indexes]\n",
    "        # créer une liste de dataframes, chacun contenant une série temporelle\n",
    "        for serie in series:\n",
    "            tmp_serie = pd.DataFrame({'displacement': pd.Series(serie, index=pd.DatetimeIndex(indexes))})\n",
    "            tmp_serie.sort_index(inplace=True)\n",
    "            df_series.append(tmp_serie)\n",
    "        # creer un dataframe pour les autres attributs\n",
    "        df = pd.DataFrame(data)\n",
    "        for column in df.columns:\n",
    "            df[column] = pd.to_numeric(df[column], errors='coerce')\n",
    "        df.set_index('id')\n",
    "        \n",
    "    return df, df_series\n",
    "\n",
    "\n",
    "def get_ids(df, column='id'):\n",
    "    \n",
    "    return df[column].tolist()\n",
    "        \n",
    "def impute(series, ids, pc=0.4):\n",
    "    \n",
    "    result = []\n",
    "    ids_to_not_keep = []\n",
    "    n_samples = len(series[0])\n",
    "    \n",
    "    for i in range(len(series)):\n",
    "        if 100*(series[i].isnull().sum().sum()/n_samples) < pc:\n",
    "            result.append(series[i].interpolate(limit_direction='both', inplace=False))\n",
    "        else:\n",
    "            ids_to_not_keep.append(ids[i])\n",
    "            \n",
    "    return result, ids_to_not_keep\n",
    "\n",
    "def imputed(serie, index=None):\n",
    "    \n",
    "    # dataframe resultat\n",
    "    result = None\n",
    "    \n",
    "    # liste des identifiants des valeurs manquantes\n",
    "    rows_with_nan = [index for index, row in serie.iterrows() if row.isnull().any()]\n",
    "    \n",
    "    if index is None:\n",
    "        result = serie.dropna(inplace=False)\n",
    "        \n",
    "    if isinstance(index, list):\n",
    "        pass\n",
    "        \n",
    "    return result, rows_with_nan\n",
    "        \n",
    "\n",
    "# imputer les données\n",
    "def impute_(ns_series, ew_series, pc=0.4):\n",
    "\n",
    "    # nombre de mésures\n",
    "    n = len(ns_series[0])\n",
    "    \n",
    "    # recupérer les series de la composante Nord-Sud qui ont un pourcentage de valeurs manquuantes inferieur à 50%\n",
    "    ns_booleans  = [True if (ns_series[i].isnull().sum().sum()/n) < pc else False for i in range(len(ns_series))]\n",
    "    # recupérer les series de la composante Est-Ouest qui ont un pourcentage de valeurs manquuantes inferieur à 50%\n",
    "    ew_booleans  = [True if (ew_series[i].isnull().sum().sum()/n) < pc else False for i in range(len(ew_series))]\n",
    "    # Conserver uniquement les points où la norme du vecteur vitesse est calculable\n",
    "    booleans     = [True if i and j else False for i, j in zip(ns_booleans, ew_booleans)]\n",
    "    \n",
    "    # suppression des séries temporelles qui ont un pourcentage de valeurs manquantes supérieure ou égale à 0.5\n",
    "    ns_series_c = [d for d in itertools.compress(ns_series, booleans)]\n",
    "    ew_series_c = [d for d in itertools.compress(ew_series, booleans)]\n",
    "    \n",
    "    # interpoler les valeurs manquantes des les séries restantes\n",
    "    for i in range(len(ns_series_c)):\n",
    "        if ns_series_c[i].isnull().sum().sum() > 0:\n",
    "            ns_series_c[i].interpolate(limit_direction='both', inplace=True)\n",
    "        if ew_series_c[i].isnull().sum().sum() > 0:\n",
    "            ew_series_c[i].interpolate(limit_direction='both', inplace=True)\n",
    "            \n",
    "    return ns_series_c[0].index, ew_series_c, ns_series_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "ed110ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "reg = linear_model.LinearRegression()\n",
    "\n",
    "serie = df_ns_ts[9]\n",
    "X = np.array([abs((serie.index[0] - serie.index[n]).days) for n in range(len(serie.index))]).reshape(-1,1)\n",
    "y = StandardScaler().fit_transform(serie)\n",
    "reg.fit(X, y)\n",
    "reg.coef_\n",
    "\n",
    "def compute_slope(serie):\n",
    "    displacement = serie['displacement']\n",
    "    days = np.array([abs((serie.index[0] - serie.index[n]).days) for n in range(len(serie.index))])\n",
    "    return  np.cov(displacement, days)[0][1] / np.var(np.array(days))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "78691261",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSerie():\n",
    "    \n",
    "    def __init__(self, id_, lat, lon, topo, serie):\n",
    "        self.id   = id_\n",
    "        self.lat  = lat\n",
    "        self.lon  = lon\n",
    "        self.topo = topo\n",
    "        self.serie = self.interpolate(serie) if self.has_null_values(serie) else serie\n",
    "        self.selected = False\n",
    "    \n",
    "    def has_null_values(self, serie):\n",
    "        return True if serie.isnull().sum().sum() > 0 else False\n",
    "    \n",
    "    def interpolate(self, serie):\n",
    "        return serie.interpolate(limit_direction='both', inplace=False)\n",
    "        \n",
    "    def set_selected(self):\n",
    "        self.selected = !self.selected\n",
    "        \n",
    "    def get_linear_reg_pval(self, alpha):\n",
    "        \n",
    "        # transformer les index en durée pour pouvoire effectuer une regression linéaire\n",
    "        X = np.array([abs((self.serie.index[0] - self.serie.index[n]).days) for n in range(len(self.serie.index))]).reshape(-1,1)\n",
    "        \n",
    "        # extraire la cible\n",
    "        y = StandardScaler().fit_transform(self.serie)\n",
    "        \n",
    "        # calculer la p-value de la regression lineaire\n",
    "        _, pval = f_regression(X,y.ravel())\n",
    "        \n",
    "        return  pval\n",
    "    \n",
    "    def set_linear_reg_pvalue(pvalue):\n",
    "        self.linear_reg_pvalue = pvalue\n",
    "        \n",
    "    def set_slope(slope):\n",
    "        self.slope = slope\n",
    "        \n",
    "    def get_slope_value(self, ref, file):\n",
    "        val = os.popen('gdallocationinfo -valonly -%s %s %f %f' % (ref, file, self.lat, self.lon)).read()\n",
    "        if len(val) == 0:\n",
    "            raise ValueError('La pente est non valide !')\n",
    "        return  float(val)\n",
    "            \n",
    "    def is_to_select(self, file, ref, min_slope, alpha):\n",
    "        slope   = self.get_slope_value(ref, file)\n",
    "        p_value = self.get_linear_reg_pval(alpha)\n",
    "        # on filtre les pentes faibles \n",
    "        if slope < min_slope:\n",
    "            return False\n",
    "        # regression linéaire non significative\n",
    "        if p_value > alpha:\n",
    "            return False\n",
    "        self.set_selected()\n",
    "        return True\n",
    "\n",
    "    def set_linear_reg_pvalue(self, pvalue):\n",
    "        self.linear_reg_pvalue = pvalue\n",
    "        \n",
    "    def load(self, displacement):\n",
    "        return serie.dropna(inplace=False)\n",
    "    \n",
    "    def has_enough_values(self, pc):\n",
    "        return True if 100*(self.serie.displacement.isnull().sum().sum()/len(self.serie)) < pc else False\n",
    "        \n",
    "    def normalize(self):\n",
    "        return StandardScaler().fit_transform(self.serie)\n",
    "        \n",
    "    def compute_inst_vel(self):\n",
    "        vels =  []\n",
    "        for i in range(1, len(self.serie)-1):\n",
    "            duration = (self.serie.index[i+1] - self.serie.index[i-1]).days\n",
    "            displacement = self.serie.iloc[i+1].values[0] - self.serie.iloc[i-1].values[0]\n",
    "            vels.append(displacement / duration)\n",
    "        # supprimer le premier et le dernier index (formule non applicable)\n",
    "        return pd.DataFrame(vels, index=self.serie.index[1:-1], columns=['vel'])\n",
    "    \n",
    "    # la copie renvoie bien un nouvel objet, il n'y a pas d'effets de bord\n",
    "    def smooth(self, ampl=2):\n",
    "        serie = self.serie.copy()\n",
    "        sigma = math.sqrt(serie.var())\n",
    "        for i in range(len(serie)):\n",
    "            if abs(serie.iloc[i].displacement) > ampl*sigma:\n",
    "                # remplacer les anomalies par des valeurs manquantes\n",
    "                serie.iloc[i, serie.columns.get_loc('displacement')]= np.nan\n",
    "        return self.deepcopy(serie.interpolate(limit_direction='both', inplace=False))\n",
    "    \n",
    "    def detect_non_moving_serie(self, alpha):\n",
    "        \n",
    "        # transformer les index en durée pour pouvoire effectuer une regression linéaire\n",
    "        X = np.array([abs((self.serie.index[0] - self.serie.index[n]).days) for n in range(len(self.serie.index))]).reshape(-1,1)\n",
    "        \n",
    "        # extraire la cible\n",
    "        y = StandardScaler().fit_transform(self.serie)\n",
    "        \n",
    "        # calculer la p-value de la regression lineaire\n",
    "        _, pval = f_regression(X,y.ravel())\n",
    "        self.set_linear_reg_pvalue(pval)\n",
    "            \n",
    "        # eliminer le bruit si la regression n'est pas significative (bourrage de zeros)\n",
    "        if pval > alpha:\n",
    "            #return self.clone(pd.DataFrame(0.0, index=self.serie.index, columns=self.serie.columns))\n",
    "            return self.deepcopy(pd.DataFrame(0.0, index=self.serie.index, columns=self.serie.columns))  \n",
    "        else:\n",
    "            #return self.clone(self.serie.copy())\n",
    "            return self.deepcopy(self.serie)\n",
    "    \n",
    "    def transform(self, alpha):\n",
    "        return self.smooth().detect_non_moving_serie(alpha)\n",
    "        \n",
    "    def deepcopy(self, serie):\n",
    "        clone = copy.deepcopy(self)\n",
    "        clone.set_serie(serie)\n",
    "        return clone\n",
    "    \n",
    "    def set_serie(self,serie):\n",
    "        self.serie = serie\n",
    "        \n",
    "    def compute_adfuller(self, ts):\n",
    "        adf_result = adfuller(ts)\n",
    "        adf_output = pd.Series(adf_result[0:4],index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n",
    "        for key, value in adf_result[4].items():\n",
    "            adf_output['Critical Value (%s)'%(key)] = value\n",
    "        return adf_output[1]\n",
    "        \n",
    "    # Il y a un souci avec cette fonction\n",
    "    def clone(self, serie):\n",
    "        return TimeSerie(self.id, self.lat, self.lon, self.topo, serie)\n",
    "    \n",
    "    def get_days(dates):\n",
    "        days = []\n",
    "        for i in range(len(dates)):\n",
    "            days.append(abs((dates[0] - dates[i]).days ))\n",
    "        return days\n",
    "\n",
    "    def get_Xy(self, serie):\n",
    "        X = get_days(serie.index)\n",
    "        y = StandardScaler().fit_transform(serie)\n",
    "        return np.array(X).reshape(-1,1), y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "69341252",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataFromImageCorrelation():\n",
    "    \n",
    "    def __init__(self, raster_folder_name, dem_filename, geo, ns, ew, pc=0.4, alpha=0.05, ref='wgs84'):\n",
    "        self.ns, self.ew  = self.load(geo, ns, ew, pc)\n",
    "        self.compute_slope_map(raster_folder_name, dem_filename)\n",
    "        self.n_cores = multiprocessing.cpu_count()\n",
    "        self.non_filtered_ids = []\n",
    "        self.magnitudes = None\n",
    "        self.pc = pc \n",
    "        self.alpha = alpha\n",
    "        self.ref = ref\n",
    "    \n",
    "    def set_slope_map_path(self, slope_map_path):\n",
    "        self.slope_map_path = slope_map_path\n",
    "        \n",
    "    def select_pixel(self, n, min_slope):\n",
    "        return (self.ns[n].is_to_select(self.slope_map_path, self.ref, min_slope, self.alpha) or \n",
    "                self.ew[n].is_to_select(self.slope_map_path, self.ref, min_slope, self.alpha))\n",
    "    \n",
    "    def load_raster(self, raster_folder_name, raster_filename):\n",
    "        return gdal.Open(raster_folder_name + '/' +  raster_filename)\n",
    "        \n",
    "    def compute_slope_map(self, raster_folder_name, dem_name):\n",
    "        dem = None\n",
    "        slope_map = None\n",
    "        slope_map_name = dem_name.split('.')[0] + '_' + 'slope_map.tif'\n",
    "        slope_map_path = raster_folder_name + '/' + slope_map_name\n",
    "        if not os.path.isfile(slope_map_path):\n",
    "            dem = gdal.Open(raster_folder_name + '/'+ dem_name)\n",
    "            slope_map = gdal.DEMProcessing(slope_map_path, dem, 'slope', computeEdges = True)\n",
    "        self.set_slope_map_path(slope_map_path)\n",
    "    \n",
    "    def empty_non_filtered_ids(self):\n",
    "        self.non_filtered_ids.clear()\n",
    "        \n",
    "    def add_non_filtered_ids(self, n):\n",
    "        self.non_filtered_ids.append(n)\n",
    "    \n",
    "    def load(self, geo, ns, ew, pc):\n",
    "        ns_r, ew_r = [], []\n",
    "        for i in range(len(ns)):\n",
    "            # vérifier que les séries temporelles comporte un pourcentage de valeurs null inférieure à pc et on des id identiques\n",
    "            if ns[i].isnull().sum().sum()/len(ns[i]) < pc:\n",
    "                ns_ts = ns[i].interpolate(limit_direction='both', inplace=False)\n",
    "                ew_ts = ew[i].interpolate(limit_direction='both', inplace=False)   \n",
    "                ns_r.append(TimeSerie(geo.iloc[i].id, geo.iloc[i].Lat, geo.iloc[i].Lon, geo.iloc[i].Topo, ns_ts))\n",
    "                ew_r.append(TimeSerie(geo.iloc[i].id, geo.iloc[i].Lat, geo.iloc[i].Lon, geo.iloc[i].Topo, ew_ts))\n",
    "        return ns_r, ew_r\n",
    "    \n",
    "    def compute_vel(self, ns_component, ew_component):\n",
    "        return math.sqrt(ns_component * ns_component + ew_component * ew_component)\n",
    "    \n",
    "    def compute_magnitude(self, boolean, min_slope=None):\n",
    "        magnitudes = []\n",
    "        ns_vel_ts = None\n",
    "        ew_vel_ts = None\n",
    "        if boolean:\n",
    "            ns_vel_ts = self.compute_inst_vels(self.ns)\n",
    "            ew_vel_ts = self.compute_inst_vels(self.ew)\n",
    "        else:\n",
    "            ns, ew    = self.transform(min_slope)\n",
    "            ns_vel_ts = self.compute_inst_vels(ns)\n",
    "            ew_vel_ts = self.compute_inst_vels(ew)\n",
    "        for i in range(len(ns_vel_ts)):\n",
    "            vels = []\n",
    "            column = ns_vel_ts[0].columns[0]\n",
    "            for ns_component, ew_component in zip(ns_vel_ts[i][column], ew_vel_ts[i][column]):\n",
    "                vels.append(self.compute_vel(ns_component, ew_component))\n",
    "            df = pd.DataFrame(vels, index=ns_vel_ts[0].index, columns=['magnitude'])\n",
    "            magnitudes.append(df)\n",
    "        return magnitudes\n",
    "            \n",
    "    def compute_inst_vel(self, ts):\n",
    "        vels =  []\n",
    "        for i in range(1, len(ts.serie)-1):\n",
    "            duration = (ts.serie.index[i+1] - ts.serie.index[i-1]).days\n",
    "            displacement = ts.serie.iloc[i+1].values[0] - ts.serie.iloc[i-1].values[0]\n",
    "            vels.append(displacement / duration)\n",
    "        return pd.DataFrame(vels, index=ts.serie.index[1:-1], columns=['vel'])\n",
    "    \n",
    "    def compute_inst_vels(self, ts):\n",
    "        with multiprocessing.Pool(self.n_cores) as p:\n",
    "            results = p.map(self.compute_inst_vel, ts)\n",
    "            return results\n",
    "        \n",
    "    def transform(self, min_slope):\n",
    "        ns, ew = [], []\n",
    "        self.empty_non_filtered_ids()\n",
    "        for n in range(len(self.ns)):\n",
    "            if self.select_pixel(n, min_slope):\n",
    "                ns.append(self.ns[n].smooth().detect_non_moving_serie(self.alpha))\n",
    "                ew.append(self.ew[n].smooth().detect_non_moving_serie(self.alpha))\n",
    "                self.add_non_filtered_ids(n)\n",
    "        return ns, ew\n",
    "    \n",
    "    # renvoyer les données pour effectuer le clustering\n",
    "    # si on n'interesse qu'à la forme, il faut normaliser les données\n",
    "    def prepare(self, min_slope):\n",
    "        self.set_magnitudes(self.compute_magnitude(False, min_slope))\n",
    "    \n",
    "    # for the moment, we only normalize velocity time series\n",
    "    # use this method to use euclidian distance\n",
    "    def normalize(self):\n",
    "        return np.array([StandardScaler().fit_transform(df).reshape(len(df)) for df in self.magnitudes])\n",
    "    \n",
    "    def set_magnitudes(self, magnitudes):\n",
    "        self.magnitudes = magnitudes\n",
    "    \n",
    "    # use this matrix to use DTW with HDBSCAN\n",
    "    def compute_similarity_matrix(self):\n",
    "        size = len(self.magnitudes)\n",
    "        distances_matrix = np.zeros(shape=(len(self.magnitudes), len(self.magnitudes)))\n",
    "        for n in range(size):\n",
    "            for m in range(n , size):\n",
    "                u = StandardScaler().fit_transform(self.magnitudes[n])\n",
    "                v = StandardScaler().fit_transform(self.magnitudes[m])\n",
    "                dist = dtw.distance(u, v)\n",
    "                distances_matrix[n, m] = dist\n",
    "                distances_matrix[m, m] = dist\n",
    "        return distances_matrix\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "23235d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Visualization():\n",
    "\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def plot_displacement(self, ts):\n",
    "        \n",
    "        days = get_days(ns_ts.index)\n",
    "        fig, ax = plt.subplots(figsize=(15,5))\n",
    "    \n",
    "        ax[0].plot(ns_ts['displacement'], color='blue', label='NS displacement (m)', marker='o', linewidth=2)\n",
    "        ax[1].plot(ew_ts['displacement'], color='orange', label='EW displacement (m)', marker='o', linewidth=2)\n",
    "        \n",
    "        plt.legend()\n",
    "        plt.legend(loc='best')\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_displacement_vel(self, ns_ts, ew_ts):\n",
    "        \n",
    "        days = get_days(ns_ts[0].index)\n",
    "\n",
    "        fig, ax_left = plt.subplots(figsize=(15,5))\n",
    "        ax_right = ax_left.twinx()\n",
    "\n",
    "        p1, = ax_left.plot(days, df_series[4]['displacement'], color='blue', label='NS displacement (m)')\n",
    "        p2, = ax_right.plot(days, df_series[4]['displacement'], color='orange', label='EW displacement(m)')\n",
    "\n",
    "        ax_left.set_xlabel(\"number of days since the first measure\")\n",
    "        ax_left.set_ylabel(\"displacement\")\n",
    "        ax_right.set_ylabel(\"velocity\")\n",
    "\n",
    "        lns = [p1, p2]\n",
    "\n",
    "        ax_left.legend(handles=lns, loc='best')\n",
    "        fig.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    def plot_disp_ns_ew(self, ns_ts, ew_ts):\n",
    "\n",
    "        fig, ax = plt.subplots(2, 1, figsize=(15,10))\n",
    "\n",
    "        ax[0].plot(ns_ts, color='blue', label='displacement (m)', marker='o', linewidth=2)\n",
    "        ax[0].set_title('NS cumulative displacement')\n",
    "        ax[0].set_xlabel('time')\n",
    "        ax[0].set_ylabel('displacement')\n",
    "        ax[0].legend()\n",
    "\n",
    "        ax[1].plot(ew_ts, color='orange', label='displacement (m)', marker='o', linewidth=2)\n",
    "        ax[1].set_title('EW cumulative displacement')\n",
    "        ax[1].set_xlabel('time')\n",
    "        ax[1].set_ylabel('displacement')\n",
    "        ax[1].legend()\n",
    "        \n",
    "        fig.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "    def plot_disp_vel(self, ns_ts, ew_ts, vels):\n",
    "\n",
    "        fig, ax = plt.subplots(3, 1, figsize=(15,10))\n",
    "\n",
    "        ax[0].plot(ns_ts, color='blue', label='displacement (m)', marker='o', linewidth=2)\n",
    "        ax[0].set_title('NS cumulative displacement')\n",
    "        ax[0].set_xlabel('time')\n",
    "        ax[0].set_ylabel('displacement')\n",
    "        ax[0].legend()\n",
    "\n",
    "        ax[1].plot(ew_ts, color='orange', label='displacement (m)', marker='o', linewidth=2)\n",
    "        ax[1].set_title('EW cumulative displacement')\n",
    "        ax[1].set_xlabel('time')\n",
    "        ax[1].set_ylabel('displacement')\n",
    "        ax[1].legend()\n",
    "\n",
    "        ax[2].plot(vels, color='green', label='velocity (m/day)', marker='o', linewidth=2)\n",
    "        ax[2].set_title('Velocity magnitude')\n",
    "        ax[2].set_xlabel('time')\n",
    "        ax[2].set_ylabel('velocity')\n",
    "        ax[2].legend()\n",
    "\n",
    "        fig.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    def plot_series(series, num_rows=4, num_cols=5, colormap='tab20'):\n",
    "        \n",
    "        plot_kwds = {'alpha' : 0.25, 's' : 10, 'linewidths':0}\n",
    "        fig, axs = plt.subplots(num_rows, num_cols, figsize=(25,25))\n",
    "        cmap = plt.get_cmap(colormap)\n",
    "        colors = [cmap(i) for i in np.linspace(0,1,num_rows*num_cols)]\n",
    "\n",
    "        for num_row in range(num_rows):\n",
    "            for num_col in range(num_cols):\n",
    "                if num_row*num_cols + num_col < len(series):\n",
    "                    axs[num_row, num_col].plot(series[num_row*num_cols + num_col], color=colors[num_row*num_cols + num_col], marker='o', markerfacecolor='white')\n",
    "                    #axs[num_row, num_col].set_title('serie: %s'%(self.names[num_row*num_cols + num_col].split('.')[0]))\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "c8545214",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-408-0508635d9d2a>, line 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-408-0508635d9d2a>\"\u001b[0;36m, line \u001b[0;32m12\u001b[0m\n\u001b[0;31m    if start <= date <= end\u001b[0m\n\u001b[0m                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class Utility():\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def get_season(self, date):\n",
    "        \n",
    "        for season in seasons:\n",
    "            name  = season[0]\n",
    "            start = season[1].replace(year=date.year)\n",
    "            end   = season[2].replace(year=date.year)\n",
    "            if start <= date <= end\n",
    "                return name\n",
    "            \n",
    "    def count_samples_per_season(self, dates):\n",
    "    \n",
    "        samples_per_season = {'winter': 0, 'spring': 0, 'summer': 0, 'autumn': 0}\n",
    "        for date in dates:\n",
    "            samples_per_season[get_season(date)] += 1\n",
    "\n",
    "        return samples_per_season\n",
    "    \n",
    "    def get_days(self, dates):\n",
    "        \n",
    "        days = []\n",
    "        for i in range(len(dates)):\n",
    "            days.append(abs((dates[0] - dates[i]).days ))\n",
    "        return days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "6a986683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chargement de la composante  Est-Ouest du mouvement du sol\n",
    "df_ew, df_ew_ts = load_data('MM_TIO_EW_31TGK_20151227_to_20200906.csv')\n",
    "# chargement de la composante Nord-Sud du mouvement du sol\n",
    "df_ns, df_ns_ts = load_data('MM_TIO_NS_31TGK_20151227_to_20200906.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d482e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "RASTER_FOLDER_PATH = 'rasters'\n",
    "DEM_FILENAME =  '31TGK_copernicus_dem.tif'\n",
    "Data = DataFromImageCorrelation(RASTER_FOLDER_PATH, DEM_FILENAME, df_ew, df_ns_ts, df_ew_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff15e6c",
   "metadata": {},
   "source": [
    "# Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775eb843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualisation des vitesses brutes (calculées à partir des déplacement brutes)\n",
    "n = 0\n",
    "raw_magnitudes = Data.compute_magnitude(True)\n",
    "viz = Visualization()\n",
    "viz.plot_disp_vel(Data.ns[n].serie, Data.ew[n].serie, raw_magnitudes[n])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05645d88",
   "metadata": {},
   "source": [
    "# Calcul des profils de vitesse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0752608",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.prepare(min_slope=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "7a8ef1d9",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-198-0e252e6238f2>, line 47)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-198-0e252e6238f2>\"\u001b[0;36m, line \u001b[0;32m47\u001b[0m\n\u001b[0;31m    plt.show()nb_lines': 50, 'nb_columns': 100,\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#from datetime import datetime\n",
    "Y = 2000\n",
    "seasons  = [('winter', date(Y,  1,  1), date(Y,  3, 20)),\n",
    "           ('spring', date(Y,  3, 21),  date(Y,  6, 20)),\n",
    "           ('summer', date(Y,  6, 21),  date(Y,  9, 22)),\n",
    "           ('autumn', date(Y,  9, 23),  date(Y, 12, 20)),\n",
    "           ('winter', date(Y, 12, 21),  date(Y, 12, 31))]\n",
    "\n",
    "        \n",
    "def plot_pie(labels, values):\n",
    "\n",
    "    fig1, ax1 = plt.subplots(figsize=(5,5))\n",
    "    ax1.pie(values, labels=labels, autopct='%1.1f%%',\n",
    "            shadow=True, startangle=90)\n",
    "    ax1.axis('equal') \n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "def plot_displacement(ns_ts, ew_ts):\n",
    "    \n",
    "    days = get_days(ns_ts.index)\n",
    "    fig, ax = plt.subplots(figsize=(15,5))\n",
    "    \n",
    "    ax.plot(ns_ts['displacement'], color='blue', label='NS displacement (m)', marker='o', linewidth=2)\n",
    "    ax.plot(ew_ts['displacement'], color='orange', label='EW displacement (m)', marker='o', linewidth=2)\n",
    "    \n",
    "    plt.legend(loc='best')\n",
    "    \n",
    "def plot_displacement_velocity(ns_ts, ew_ts):\n",
    "\n",
    "    days = get_days(ns_ts[0].index)\n",
    "\n",
    "    fig, ax_left = plt.subplots(figsize=(15,5))\n",
    "    ax_right = ax_left.twinx()\n",
    "\n",
    "    p1, = ax_left.plot(days, df_series[4]['displacement'], color='blue', label='NS displacement (m)')\n",
    "    p2, = ax_right.plot(days, df_series[4]['displacement'], color='orange', label='EW displacement(m)')\n",
    "\n",
    "    ax_left.set_xlabel(\"number of days since the first measure\")\n",
    "    ax_left.set_ylabel(\"displacement\")\n",
    "    ax_right.set_ylabel(\"velocity\")\n",
    "\n",
    "    lns = [p1, p2]\n",
    "    \n",
    "    ax_left.legend(handles=lns, loc='best')\n",
    "    fig.tight_layout()\n",
    "    plt.show()nb_lines': 50, 'nb_columns': 100,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbc1d4c",
   "metadata": {},
   "source": [
    "# Clustering par Densité"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332401af",
   "metadata": {},
   "source": [
    "## HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "92a07e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Clustering():\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.result = None\n",
    "        self.n_clusters = None\n",
    "    \n",
    "    def cluster(self, min_cluster_size, precomputed=False):\n",
    "        if not precomputed: \n",
    "            self.result = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size, gen_min_span_tree=True).fit(self.data.normalize())\n",
    "        else:\n",
    "            print('lll')\n",
    "            self.result = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size, gen_min_span_tree=True, metric='precomputed').fit(self.data.compute_similarity_matrix())\n",
    "\n",
    "    def get_n_clusters(self):\n",
    "        return 0 if self.result.labels_.max() < 0 else self.result.labels_.max() + 1\n",
    "    \n",
    "    def visualize(self):\n",
    "        projection = TSNE().fit_transform(self.data.normalize())\n",
    "        color_palette = sns.color_palette('Paired', self.result.labels_.max() + 1 )\n",
    "        cluster_colors = [color_palette[x] if x >= 0\n",
    "                  else (0.5, 0.5, 0.5)\n",
    "                  for x in self.result.labels_]\n",
    "        cluster_member_colors = [sns.desaturate(x, p) for x, p in\n",
    "                         zip(cluster_colors, self.result.probabilities_)]\n",
    "        plt.scatter(*projection.T, s=20, c=cluster_member_colors , linewidth=0, alpha=0.25)\n",
    "    plt.show()\n",
    "        \n",
    "    def plot_cluster_distribution(self, colormap='Dark2'):\n",
    "        v_count = dict()\n",
    "        cmap = plt.get_cmap(colormap)\n",
    "        colors = [cmap(i) for i in np.linspace(0, 1, self.result.labels_.max() + 1)]\n",
    "        for label in self.result.labels_:\n",
    "            if label != -1:\n",
    "                if label in v_count.keys():\n",
    "                    v_count[label] += 1\n",
    "                else:\n",
    "                    v_count[label] = 1\n",
    "        pd.Series({k: v for k, v in sorted(v_count.items(), key=lambda item: item[0])}).plot(kind='bar', color=colors)\n",
    "        return v_count\n",
    "    \n",
    "    def get_data_from_class(self, num_label):\n",
    "        data = []\n",
    "        for i in range(len(self.result.labels_)):\n",
    "            if self.result.labels_[i] == num_label:\n",
    "                data.append(self.data[i])\n",
    "        return data\n",
    "    \n",
    "    def plot_cluster_result(self, n_cols=3):\n",
    "        labels = self.result.labels_\n",
    "        n_clusters = labels.max() + 1\n",
    "        n_rows = int(n_clusters / n_cols) if n_clusters % n_cols == 0 else int(math.ceil(n_clusters / n_cols))\n",
    "        fig, axs = plt.subplots(n_rows, n_cols, figsize=(25,15))\n",
    "        for num_cluster in range(n_clusters):\n",
    "            for serie_index in range(len(self.data.magnitudes)):\n",
    "                if labels[serie_index] == num_cluster:\n",
    "                    axs[int(num_cluster / n_cols), num_cluster % n_cols].plot(self.data.magnitudes[serie_index].serie, c='blue', alpha=0.2)\n",
    "            axs[int(num_cluster / n_cols), num_cluster % n_cols].set_title('Cluster %d'%(num_cluster + 1))\n",
    "        fig.tight_layout()\n",
    "        \n",
    "    # créer un fichier csv contenant les champs : id, lat, lon, numero de la classe\n",
    "    def save_result(self):\n",
    "\n",
    "        fieldnames = ['id', 'Lat', 'Lon', 'cluster']\n",
    "        rows = []\n",
    "        \n",
    "        for n in range(len(self.data.ns)):\n",
    "            rows.append({'id': int(self.data.ns[n].id), 'Lat': self.data.ns[n].lat, 'Lon': self.data.ns[n].lon, 'cluster': self.result.labels_[n]})\n",
    "\n",
    "        with open('clustering_result.csv', 'w', encoding='UTF8', newline='') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            writer.writerows(rows)\n",
    "            \n",
    "    def generate_kml_file(self, scale=2):\n",
    "        labels = self.result.labels_\n",
    "        if labels.max() > -1:\n",
    "            icon_links, scales = self.generate_icones(labels)\n",
    "            kml=simplekml.Kml()\n",
    "            fol = kml.newfolder(name=\"HDBSCAN Clustering\")\n",
    "            labels = self.result.labels_\n",
    "            for i, n in enumerate(self.data.non_filtered_ids, 0):\n",
    "                if labels[i] > - 1:\n",
    "                    pnt = fol.newpoint(coords=[(self.data.ns[n].lat, self.data.ns[n].lon)])\n",
    "                    pnt.iconstyle.icon.href = icon_links[labels[i]]\n",
    "                    pnt.style.labelstyle.scale = scales[labels[i]]\n",
    "            kml.save('clustering_result.kml')\n",
    "        else:\n",
    "            print('Hdbscan only found outliers. kml file cannot be generated !')\n",
    "    \n",
    "    #http://tancro.e-central.tv/grandmaster/markers/google-icons/mapfiles-ms-micons.html\n",
    "    def generate_icones(self, labels, scale=2):\n",
    "        BASE = 'http://maps.google.com/mapfiles/ms/micons/'\n",
    "        scales = None\n",
    "        colors = ['blue', 'red', 'yellow', 'green', 'orange', 'purple', 'pink']\n",
    "        icon_links = [BASE + color + '-dot.png' for color in (colors + ['Itblue'])[:]] + [BASE + color + '.png' for color in (colors + ['lightblue'])[:]]\n",
    "        n_clusters = labels.max() + 1\n",
    "        if n_clusters > len(icon_links):\n",
    "            q = int(n_clusters / len(icon_links))\n",
    "            r = n_clusters - q*len(icon_links)\n",
    "            icon_links = (icon_links*q)[:] + icon_links[:r]\n",
    "            scales = [scale*t for t in range(len(icon_links))]\n",
    "        else:\n",
    "            scales = [scale]*n_clusters\n",
    "        return icon_links, scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b70167d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import simplekml\n",
    "clustering = Clustering(Data)\n",
    "clustering.cluster(min_cluster_size=30, precomputed=False)\n",
    "clustering.visualize()\n",
    "clustering.generate_kml_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e9ba9e26",
   "metadata": {},
   "outputs": [],
   "source": [
    " def get_pval(serie, alpha=0.05):\n",
    "        \n",
    "        # transformer les index en durée pour pouvoire effectuer une regression linéaire\n",
    "        X = np.array([abs((serie.index[0] - serie.index[n]).days) for n in range(len(serie.index))]).reshape(-1,1)\n",
    "        \n",
    "        # extraire la cible\n",
    "        y = StandardScaler().fit_transform(serie)\n",
    "        \n",
    "        # calculer la p-value de la regression lineaire\n",
    "        _, pval = f_regression(X,y.ravel())\n",
    "        \n",
    "        if pval > alpha:\n",
    "            print('non significative')\n",
    "        else:\n",
    "            print('significative')\n",
    "            \n",
    "def smooth(serie_, ampl=2):\n",
    "    serie = serie_.copy()\n",
    "    sigma = math.sqrt(serie.var())\n",
    "    for i in range(len(serie)):\n",
    "        if abs(serie.iloc[i].displacement) > ampl*sigma:\n",
    "            # remplacer les anomalies par des valeurs manquantes\n",
    "            serie.iloc[i, serie.columns.get_loc('displacement')]= np.nan\n",
    "    return serie.interpolate(limit_direction='both', inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9834e0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "significative\n"
     ]
    }
   ],
   "source": [
    "#s = pd.DataFrame(0.0, index=Data.ns[n].serie.index, columns=['pp'])\n",
    "get_pval(Data.ew[n].serie)\n",
    "# attention il y a bug quelque part, il faut s'assurer de calculer la regression uniquement après avoir lisser\n",
    "# les pics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ef3750",
   "metadata": {},
   "outputs": [],
   "source": [
    "viz = Visualization()\n",
    "index = Data.non_filtered_ids\n",
    "n = 2928\n",
    "viz.plot_disp_ns_ew(Data.ns[n].serie, Data.ew[n].serie)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8ead20",
   "metadata": {},
   "source": [
    "# ICA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16c4a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape(data):\n",
    "    n_pixel =  len(data)\n",
    "    n_samples = len(data[0].serie)\n",
    "    return  np.array(data).reshape(n_pixels, n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "77e3250e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_distances_matrix(series):\n",
    "    \n",
    "    # initialisation de la matrix des distances\n",
    "    distances_matrix = np.zeros(shape=(len(series), len(series)))\n",
    "    \n",
    "    # calcul des distances et remplissage de la matrice de distance\n",
    "    for n in range(len(series)):\n",
    "        for m in range(len(series)):\n",
    "            x = series[n]['velocity'].dropna(inplace=False)\n",
    "            y = series[m]['velocity'].dropna(inplace=False)\n",
    "            dist = dtw.distance(x, y)\n",
    "            distances_matrix[n, m] = dist\n",
    "    return distances_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "5bc1c5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With ICA\n",
    "# S = W X\n",
    "# S_ = fastICA().fit_transform(X)\n",
    "# W : matrice des poids\n",
    "# X : matrice des observations. X.shape = (n_samples, n_sources)\n",
    "# S: les sources\n",
    "# S_ : les sources reconstitués . S_.shape = X.shape\n",
    "\n",
    "# With PCA :\n",
    "# r = PCA().fit_transform(X)\n",
    "# r: les sources reconstutées avec l'ACP\n",
    "\n",
    "# temporal ICA \n",
    "# X = A S\n",
    "# X.shape = (n_pixel, n_samples)\n",
    "# A.shape = (n_source, n_pixel)\n",
    "# S.shape = (n_source, n_samples)\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.decomposition import FastICA\n",
    "\n",
    "def reshape(data):\n",
    "    n_pixels =  len(data)\n",
    "    n_samples = len(data[0])\n",
    "    return  np.array(data).reshape(n_pixels, n_samples)\n",
    "X = reshape(Data.magnitudes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbc9df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comment choisir sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d72a4d",
   "metadata": {},
   "source": [
    "# Significativité des régressions linéaires\n",
    "* p-value : probabilité que la pente soit nulle\n",
    "* Hypothèse nulle HO : la pente de la droite de regression est nulle  (vitesse moyenne non significative)\n",
    "* si la p-value est inférieure à 0.05 alors l'hypothèse nulle est rejetée (vitesse moyenne est significative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8cc7e66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from statsmodels.api import add_constant\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5be0170f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_days(dates):\n",
    "    days = []\n",
    "    for i in range(len(dates)):\n",
    "        days.append(abs((dates[0] - dates[i]).days ))\n",
    "    return days\n",
    "\n",
    "def get_xy(serie):\n",
    "    X = get_days(serie.index)\n",
    "    y = StandardScaler().fit_transform(serie)\n",
    "    return np.array(X).reshape(-1,1), y\n",
    "\n",
    "def compute_linear_regression(X,y):\n",
    "    \n",
    "    X_ = add_constant(X)\n",
    "    mod = sm.OLS(y,X_)\n",
    "    fii = mod.fit()\n",
    "    \n",
    "    return fii.params, fii.pvalues\n",
    "\n",
    "def compute_slope(displacement, days):\n",
    "    \n",
    "    return  np.cov(displacement, days)[0][1] / np.var(np.array(days))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ab394c",
   "metadata": {},
   "source": [
    "## Test de Dickey-Fuller augmenté (ADF)\n",
    "La série temporelle est considérée comme stationnaire si la valeur p est faible (selon l’hypothèse nulle) et si les valeurs critiques à des intervalles de confiance de 1%, 5%, 10% sont aussi proches que possible des statistiques de l’ADF (Augmented Dickey-Fuller)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6f1541",
   "metadata": {},
   "source": [
    "# Réechantillonage des données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e50929",
   "metadata": {},
   "source": [
    "# Etude de la saisonnalité"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
