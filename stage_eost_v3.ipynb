{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a7791d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from datetime import date, datetime\n",
    "import datetime\n",
    "from tslearn.barycenters import dtw_barycenter_averaging\n",
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA, FastICA\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from scipy.spatial.distance import euclidean\n",
    "from sklearn.feature_selection import f_regression\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from sklearn.manifold import TSNE\n",
    "from DBCV import DBCV\n",
    "import hdbscan\n",
    "import csv\n",
    "import multiprocessing\n",
    "import itertools\n",
    "import operator\n",
    "import math\n",
    "from dtaidistance import dtw\n",
    "import seaborn as sns\n",
    "import elevation\n",
    "import json\n",
    "plt.style.use('fivethirtyeight')\n",
    "from osgeo import gdal \n",
    "from subprocess import Popen\n",
    "import simplekml\n",
    "import copy\n",
    "from sklearn.cluster import DBSCAN\n",
    "import numpy.ma as ma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "e884e3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data():\n",
    "    \n",
    "    def __init__(self, data_path, filename1, filename2, pc=0.3):\n",
    "        self.load(data_path, filename1, filename2, pc)\n",
    "        \n",
    "    def set_latitudes(self, latitudes):\n",
    "        self.latitudes = latitudes\n",
    "        \n",
    "    def set_longitudes(self, longitudes):\n",
    "        self.longitudes = longitudes\n",
    "        \n",
    "    def set_topo(self, topo):\n",
    "        self.topo = topo\n",
    "        \n",
    "    def set_ns_mean_velocities(self, velocities):\n",
    "        self.ns_mean_velocities = velocities\n",
    "        \n",
    "    def set_ew_mean_velocities(self, velocities):\n",
    "        self.ew_mean_velocities = velocities\n",
    "    \n",
    "    def set_dates(self, dates):\n",
    "        self.dates = dates\n",
    "    \n",
    "    def set_ns_displacements(self, ns_displacements):\n",
    "        self.ns_displacements = ns_displacements\n",
    "        \n",
    "    def set_ew_displacements(self, ew_displacements):\n",
    "        self.ew_displacements = ew_displacements\n",
    "        \n",
    "    def load(self,  data_path, filename1, filename2, pc):\n",
    "        ns_displacements, ew_displacements, booleans = [], [], []\n",
    "        ns_infos, ns = self.load_component(data_path, filename1)\n",
    "        ew_infos, ew = self.load_component(data_path, filename2)\n",
    "        m = len(ns[0])\n",
    "        \n",
    "        for n, components in enumerate(zip(ns, ew)):\n",
    "            if ns[n].isnull().sum().sum() / m < pc:\n",
    "                ns_displacements.append(components[0].interpolate(limit_direction='both', inplace=False)['displacement'].values)\n",
    "                ew_displacements.append(components[1].interpolate(limit_direction='both', inplace=False)['displacement'].values)\n",
    "                booleans.append(False)\n",
    "            else:\n",
    "                booleans.append(True)\n",
    "            \n",
    "        self.set_latitudes(ma.array(ns_infos['Lat'].values, mask = booleans).compressed())\n",
    "        self.set_longitudes(ma.array(ns_infos['Lon'].values, mask = booleans).compressed())\n",
    "        self.set_topo(ma.array(ns_infos['Topo'].values, mask = booleans).compressed())\n",
    "        self.set_ns_mean_velocities(ma.array(ns_infos['Vel'].values, mask = booleans).compressed())\n",
    "        self.set_ew_mean_velocities(ma.array(ew_infos['Vel'].values, mask = booleans).compressed())\n",
    "        self.set_ns_displacements(np.array(ns_displacements))\n",
    "        self.set_ew_displacements(np.array(ew_displacements))\n",
    "        self.set_dates(ns[0].index)\n",
    "        \n",
    "    #np.count_nonzero(np.isnan(data))\n",
    "    def load_image_correlation(self, data_path, ns_fi, ew_filename):\n",
    "        df_ns, df_ns_ts = self.load_component(data_path, ns_filename)\n",
    "        df_ew, df_ew_ts = self.load_component(data_path, ew_filename)\n",
    "        df_ew.rename(columns={'Vel': 'Vel_ew'}, inplace=True)\n",
    "        df_ns.rename(columns={'Vel': 'Vel_ns'}, inplace=True)\n",
    "        geo = pd.concat([df_ew[['id', 'Lat','Lon','Topo','Vel_ew']], df_ns[['Vel_ns']]], axis=1)\n",
    "        return geo, df_ns_ts, df_ew_ts\n",
    "        \n",
    "    def load_component(self, data_path, filename):\n",
    "        \n",
    "        # numéro de la ligne ou commence les données\n",
    "        num_start = 44\n",
    "        # numéro de la ligne ou se trouve la liste des dates\n",
    "        num_list_dates = 40\n",
    "        # attributs présent dans les données\n",
    "        columns = ['id', 'Lat','Lon', 'Topo', 'Vel', 'Coer',' CosN', 'CosE', 'CosU']\n",
    "        # dictionnaire stockant les données\n",
    "        data = {column: [] for column in columns}\n",
    "        # liste des dates \n",
    "        indexes = []\n",
    "        # series temporelles\n",
    "        series = []\n",
    "        # liste de dataframes\n",
    "        df_series = []\n",
    "\n",
    "        with open(data_path + '/' + filename) as csv_file:\n",
    "            csv_reader = csv.reader(csv_file, delimiter=\",\")\n",
    "            line_count = 1 \n",
    "            for row in csv_reader:\n",
    "                if line_count == num_list_dates:\n",
    "                    indexes = [row[0].split(' ')[1]] + row[1:]\n",
    "                if line_count >= num_start:\n",
    "                    # extraction des premiers attributs\n",
    "                    for i in range(len(columns)):\n",
    "                        data[columns[i]].append(row[i])\n",
    "                    # extraction de l'attribut TS(série temporelle)\n",
    "                    series.append([float(v) for v in row[len(columns):]])\n",
    "                line_count  += 1\n",
    "            if len(indexes) != len(series[0]):\n",
    "                print('Erreur : Les indexes et les valeurs ne correspondent pas')\n",
    "            # convertir les index en date\n",
    "            indexes = [d.strip()[0:8] for d in indexes]\n",
    "            # créer une liste de dataframes, chacun contenant une série temporelle\n",
    "            for serie in series:\n",
    "                tmp_serie = pd.DataFrame({'displacement': pd.Series(serie, index=pd.DatetimeIndex(indexes))})\n",
    "                tmp_serie.sort_index(inplace=True)\n",
    "                df_series.append(tmp_serie)\n",
    "            # creer un dataframe pour les autres attributs\n",
    "            df = pd.DataFrame(data)\n",
    "            for column in df.columns:\n",
    "                df[column] = pd.to_numeric(df[column], errors='coerce')\n",
    "            df.set_index('id')\n",
    "            \n",
    "        return df, df_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931ffb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSerie():\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def has_null_values(self, serie):\n",
    "        return self.count_null_values(serie) > 0\n",
    "    \n",
    "    def count_null_values(self, serie):\n",
    "        return serie.isnull().sum().sum()\n",
    "    \n",
    "    def compute_null_val_percentage(self, serie):\n",
    "        return 100 *(1.0 * self.count_null_values(serie) / len(serie))\n",
    "    \n",
    "    def interpolate(self, serie):\n",
    "        return serie.interpolate(limit_direction='both', inplace=False)\n",
    "    \n",
    "    def compute_pearson_coef(self, serie):\n",
    "        return stats.pearsonr(np.squeeze(serie.values), get_days(serie.index))\n",
    "    \n",
    "    def compute_linear_reg_pval(self, serie):\n",
    "        # extraire X et y\n",
    "        X, y = self.prepare(serie)\n",
    "        # calculer la p-value de la regression lineaire\n",
    "        _, pval = f_regression(X,y.ravel())\n",
    "        return  pval[0]\n",
    "        \n",
    "    def select(self, serie, filename, ref, min_slope, alpha, sigma, ampl, pc):\n",
    "        slope   = self.get_slope_value(ref, serie, filename)\n",
    "        p_value = self.get_linear_reg_pval(serie, alpha)\n",
    "        vlm = self.vlm\n",
    "        # filtrage des series avec peu de valeurs\n",
    "        if self.compute_nul_val_percentage > pc:\n",
    "            return False\n",
    "        # filtrage des regressions non significatives\n",
    "        if p_value > alpha:\n",
    "            return False\n",
    "        # filtrage des vitesses faibles\n",
    "        if abs(vlm) < ampl* sigma:\n",
    "            return False\n",
    "         # filtrage des pentes faibles\n",
    "        if slope < min_slope:\n",
    "            return False\n",
    "        # sauvegarder l'état du pixel\n",
    "        self.set_selected()\n",
    "        return True\n",
    "    \n",
    "    # la copie renvoie bien un nouvel objet, il n'y a pas d'effets de bord\n",
    "    def smooth(self, s, ampl):\n",
    "        serie = s.copy()\n",
    "        std = math.sqrt(serie.var())\n",
    "        for i in range(len(serie)):\n",
    "            if abs(serie.iloc[i].displacement) > ampl*std:\n",
    "                serie.iloc[i, serie.columns.get_loc('displacement')]= np.nan\n",
    "        return serie.interpolate(limit_direction='both', inplace=False)\n",
    "    \n",
    "        \n",
    "    def deepcopy(self, serie):\n",
    "        clone = copy.deepcopy(self)\n",
    "        clone.set_serie(serie)\n",
    "        return clone\n",
    "        \n",
    "    def compute_adfuller(self, serie):\n",
    "        adf_result = adfuller(serie)\n",
    "        adf_output = pd.Series(adf_result[0:4],index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n",
    "        for key, value in adf_result[4].items():\n",
    "            adf_output['Critical Value (%s)'%(key)] = value\n",
    "        return adf_output[1]\n",
    "    \n",
    "    # ce test nécessite d'avoir des données regulièrement échantillonnées\n",
    "    # Hyopthèse nulle : il existe une racine unitaire (série croissante ou cyclique)\n",
    "    # Hypthèse alternative : il n'existe pas de racine unitaire (série stationnaire)\n",
    "    # Si la p-valeur du test est inférireure à O.05, on rejette l'hypothèse nulle et la sériee est stationnaire\n",
    "    # NB: on recherche des signaux non stationnaires \n",
    "    def is_stationary(self, serie, freq='D', alpha=0.05):\n",
    "        resampled = serie.resample(freq)\n",
    "        interpolated = upsampled.interpolate(method='linear')\n",
    "        return self.compute_adfuller(interpolated) < alpha\n",
    "    \n",
    "    def get_days(self, serie):\n",
    "        days = []\n",
    "        dates = serie.index\n",
    "        for i in range(len(dates)):\n",
    "            days.append(abs((dates[0] - dates[i]).days ))\n",
    "        return days\n",
    "    \n",
    "    def prepare(self, serie):\n",
    "        # transformer les index en durée pour pouvoire effectuer une regression linéaire\n",
    "        X = np.array([abs((serie.index[0] - serie.index[n]).days) for n in range(len(serie.index))]).reshape(-1,1)\n",
    "        # extraire la cible\n",
    "        y = StandardScaler().fit_transform(serie)\n",
    "        return X, y\n",
    "    \n",
    "    # that functions gives approximately the same result when use sklearn linear regression\n",
    "    # x and y and numpy array\n",
    "    def compute_slope(self, serie):\n",
    "        x, y = self.prepare(serie)\n",
    "        return np.cov(x.T, y.T)[0][1] / np.var(x)\n",
    "    \n",
    "    def compute_inst_vel(self, serie):\n",
    "        vels =  []\n",
    "        for i in range(1, len(serie)-1):\n",
    "            duration = (serie.index[i+1] - serie.index[i-1]).days\n",
    "            displacement = serie.iloc[i+1].values[0] - serie.iloc[i-1].values[0]\n",
    "            vels.append(displacement / duration)\n",
    "        return pd.DataFrame(vels, index=serie.index[1:-1], columns=['vel'])\n",
    "        \n",
    "    def compute_diff_vect(self, serie):\n",
    "        disp     = np.diff(np.squeeze(serie.values))\n",
    "        duration = np.diff(np.squeeze(serie.index)) /  np.timedelta64(1,'D')\n",
    "        return disp, np.cumsum(duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8c5245",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSerieProcessing():\n",
    "    def __init__(self):\n",
    "        \n",
    "    def is_linear_reg_significant(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "644fd95b",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-126-e30ad02c959b>, line 17)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-126-e30ad02c959b>\"\u001b[0;36m, line \u001b[0;32m17\u001b[0m\n\u001b[0;31m    def compute_mean_velocity(self, n)\u001b[0m\n\u001b[0m                                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class ImageCorrelation():\n",
    "    \n",
    "    def __init__(sel, data, pc=0.4, alpha=0.05, ref='wgs84'):\n",
    "        self.data = data\n",
    "        self.alpha = alpha\n",
    "        self.ref = ref\n",
    "        self.pc = pc\n",
    "        self.velocities = None\n",
    "        self.mask = None\n",
    "    \n",
    "    def reshape(self):\n",
    "        pass \n",
    "    \n",
    "    def compute_std_velocities():\n",
    "        pass \n",
    "    \n",
    "    def compute_mean_velocity(self, n)\n",
    "        ns_vel = self.data.ns_mean_velocities[n]\n",
    "        ew_vel = self.data.ew_mean_velocities[n]\n",
    "        return np.sqrt(ns_vel * ns_vel + ew_vel * ew_vel)\n",
    "    \n",
    "    def is_moving(self, n):\n",
    "        pass\n",
    "    \n",
    "    def is_linear_regression_significant(self):\n",
    "        pass\n",
    "    \n",
    "    def is_steep():\n",
    "        pass\n",
    "    \n",
    "    def compute_slope(self):\n",
    "        pass\n",
    "    \n",
    "    def is_to_select(self):\n",
    "        pass\n",
    "        \n",
    "        \n",
    "    def compute_velocities():\n",
    "        pass\n",
    "    \n",
    "    def remove():\n",
    "        pass\n",
    "    \n",
    "    def set_velocities(self):\n",
    "        pass\n",
    "    \n",
    "    # amplitude, alpha, min_slope\n",
    "    def set_filter_parameters(params):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c6defd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Clustering():\n",
    "    \n",
    "    def __init__(self, data, velocities, mask, option=0):\n",
    "        self.data = data\n",
    "        self.velocities = velocities\n",
    "        self.mask = mask\n",
    "        self.option = option\n",
    "        \n",
    "    # stocker les résultats du clustering\n",
    "    def generate_kml_file(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "66a768fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = './donnees' \n",
    "filename1  = 'MM_TIO_NS_31TGK_20151227_to_20200906.csv'\n",
    "filename2  = 'MM_TIO_EW_31TGK_20151227_to_20200906.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "905dce0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Data(DATA_PATH, filename1, filename2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "f7b21499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'latitudes': array([6.627922, 6.628048, 6.628173, ..., 6.662486, 6.662611, 6.662736]),\n",
       " 'longitudes': array([44.422007, 44.422003, 44.421999, ..., 44.394165, 44.394161,\n",
       "        44.394157]),\n",
       " 'topo': array([1710.  , 1714.93, 1720.07, ..., 1280.52, 1277.93, 1275.  ]),\n",
       " 'ns_mean_velocities': array([-0.0009, -0.0009, -0.001 , ..., -0.    , -0.    , -0.    ]),\n",
       " 'ew_mean_velocities': array([-0.0003, -0.0003, -0.0003, ..., -0.0025, -0.0025, -0.0025]),\n",
       " 'ns_displacements': array([[ 0.    , -1.4829, -1.1083, ..., -1.6207, -0.747 , -1.7681],\n",
       "        [ 0.    , -1.5048, -1.135 , ..., -1.6455, -0.7691, -1.8022],\n",
       "        [ 0.    , -1.471 , -1.1931, ..., -1.7787, -0.8391, -1.9032],\n",
       "        ...,\n",
       "        [ 0.    , -0.4363,  2.7005, ..., -3.4003,  4.2267, -3.3809],\n",
       "        [ 0.    , -0.4398,  2.6997, ..., -3.4046,  4.2232, -3.3834],\n",
       "        [ 0.    , -0.4432,  2.6989, ..., -3.4085,  4.2196, -3.3856]]),\n",
       " 'ew_displacements': array([[ 0.0000e+00, -6.3500e-02, -6.6910e-01, ..., -4.0860e-01,\n",
       "          2.0200e-02,  1.3560e-01],\n",
       "        [ 0.0000e+00, -7.2000e-02, -6.8950e-01, ..., -4.2190e-01,\n",
       "         -4.6000e-03,  1.1610e-01],\n",
       "        [ 0.0000e+00, -5.0300e-02, -6.8960e-01, ..., -4.6530e-01,\n",
       "          6.1000e-03,  2.4000e-03],\n",
       "        ...,\n",
       "        [ 0.0000e+00, -1.3549e+00, -3.9013e+00, ..., -2.9631e+00,\n",
       "         -1.8962e+00, -8.4100e-01],\n",
       "        [ 0.0000e+00, -1.3575e+00, -3.9020e+00, ..., -2.9618e+00,\n",
       "         -1.8932e+00, -8.3820e-01],\n",
       "        [ 0.0000e+00, -1.3598e+00, -3.9026e+00, ..., -2.9604e+00,\n",
       "         -1.8904e+00, -8.3560e-01]]),\n",
       " 'dates': DatetimeIndex(['2015-12-27', '2016-04-25', '2016-06-24', '2016-08-03',\n",
       "                '2016-08-13', '2016-08-23', '2016-09-02', '2016-09-22',\n",
       "                '2017-02-19', '2017-03-11', '2017-03-21', '2017-04-10',\n",
       "                '2017-06-19', '2017-07-04', '2017-07-14', '2017-07-29',\n",
       "                '2017-08-18', '2017-08-23', '2017-09-07', '2017-10-07',\n",
       "                '2017-10-12', '2017-10-17', '2017-10-27', '2017-11-16',\n",
       "                '2017-11-21', '2017-11-26', '2017-12-06', '2017-12-21',\n",
       "                '2018-02-09', '2018-04-25', '2018-05-25', '2018-06-19',\n",
       "                '2018-07-04', '2018-07-09', '2018-07-19', '2018-07-24',\n",
       "                '2018-07-29', '2018-08-23', '2018-08-28', '2018-09-12',\n",
       "                '2018-09-27', '2018-10-02', '2018-10-22', '2018-11-16',\n",
       "                '2018-12-11', '2018-12-31', '2019-01-05', '2019-01-15',\n",
       "                '2019-01-20', '2019-01-25', '2019-02-14', '2019-02-24',\n",
       "                '2019-03-31', '2019-04-30', '2019-05-15', '2019-05-30',\n",
       "                '2019-06-04', '2019-06-19', '2019-06-29', '2019-07-04',\n",
       "                '2019-07-19', '2019-07-29', '2019-08-03', '2019-08-08',\n",
       "                '2019-08-18', '2019-09-12', '2019-09-17', '2019-09-27',\n",
       "                '2019-10-07', '2019-12-31', '2020-01-05', '2020-02-09',\n",
       "                '2020-02-14', '2020-02-24', '2020-03-15', '2020-04-04',\n",
       "                '2020-04-09', '2020-04-14', '2020-04-24', '2020-04-29',\n",
       "                '2020-05-04', '2020-06-23', '2020-07-23', '2020-08-07',\n",
       "                '2020-08-27', '2020-09-01', '2020-09-06'],\n",
       "               dtype='datetime64[ns]', freq=None)}"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "41bce1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "duration = np.diff(data.dates) / np.timedelta64(1,'D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "684a58d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2021, 7, 1, 14, 54, 28, 780746)"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "a20a39cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "duration = np.insert(np.cumsum( np.diff(data.dates) /  np.timedelta64(1,'D') ), 0, 0, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "03a4f20f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0.,  120.,  180.,  220.,  230.,  240.,  250.,  270.,  420.,\n",
       "        440.,  450.,  470.,  540.,  555.,  565.,  580.,  600.,  605.,\n",
       "        620.,  650.,  655.,  660.,  670.,  690.,  695.,  700.,  710.,\n",
       "        725.,  775.,  850.,  880.,  905.,  920.,  925.,  935.,  940.,\n",
       "        945.,  970.,  975.,  990., 1005., 1010., 1030., 1055., 1080.,\n",
       "       1100., 1105., 1115., 1120., 1125., 1145., 1155., 1190., 1220.,\n",
       "       1235., 1250., 1255., 1270., 1280., 1285., 1300., 1310., 1315.,\n",
       "       1320., 1330., 1355., 1360., 1370., 1380., 1465., 1470., 1505.,\n",
       "       1510., 1520., 1540., 1560., 1565., 1570., 1580., 1585., 1590.,\n",
       "       1640., 1670., 1685., 1705., 1710., 1715.])"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "9c156283",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "def is_mean_velocity_significant(displacements, days):\n",
    "    # extraire X et y\n",
    "    X = StandardScaler().fit_transform(displacements.reshape(-1,1))\n",
    "    # calculer la p-value de la regression lineaire\n",
    "    _, pval = f_regression(X, days)\n",
    "    return  pval[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "9a0a7b29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False,  True,  True,  True])"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test(x,y):\n",
    "    return x > y\n",
    "L = np.array([2,3,9,10,11])\n",
    "test(L,y=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "bdaad459",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,  120,  180,  220,  230,  240,  250,  270,  420,  440,  450,\n",
       "        470,  540,  555,  565,  580,  600,  605,  620,  650,  655,  660,\n",
       "        670,  690,  695,  700,  710,  725,  775,  850,  880,  905,  920,\n",
       "        925,  935,  940,  945,  970,  975,  990, 1005, 1010, 1030, 1055,\n",
       "       1080, 1100, 1105, 1115, 1120, 1125, 1145, 1155, 1190, 1220, 1235,\n",
       "       1250, 1255, 1270, 1280, 1285, 1300, 1310, 1315, 1320, 1330, 1355,\n",
       "       1360, 1370, 1380, 1465, 1470, 1505, 1510, 1520, 1540, 1560, 1565,\n",
       "       1570, 1580, 1585, 1590, 1640, 1670, 1685, 1705, 1710, 1715])"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_days(index):\n",
    "    days = []\n",
    "    dates = index\n",
    "    for i in range(len(dates)):\n",
    "        days.append(abs((dates[0] - dates[i]).days ))\n",
    "    return days\n",
    "\n",
    "np.array(get_days(data.dates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "ac98762d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "0898e219",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_(ns_displacements, ew_displacements):\n",
    "    data = []\n",
    "    for n in range(len(ns_displacements)):\n",
    "        data.append(np.vstack((ns_displacements[n], ns_displacements[n])).T)\n",
    "    return np.array(data)\n",
    "\n",
    "\n",
    "def apply_pca(data):\n",
    "    scaler = StandardScaler()\n",
    "    return scaler.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "03670fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = reshape_(data.ns_displacements, data.ew_displacements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "a7fc3776",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape2(data):\n",
    "    n = len(data[0])\n",
    "    output = []\n",
    "    for d in data:\n",
    "        output.append(StandardScaler().fit_transform(d).reshape(n))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "4a584316",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(87016, 87)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.ns_displacements.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "c184b40c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-9.87062760e+00,  9.04401016e+00,  1.51967271e+00, ...,\n",
       "        -1.13453654e-01,  3.83985465e-02,  5.47749049e-13],\n",
       "       [-1.00801857e+01,  9.00324727e+00,  1.59106049e+00, ...,\n",
       "        -1.12404092e-01,  3.08430930e-02, -7.98116546e-17],\n",
       "       [-1.05044376e+01,  8.74962331e+00,  2.03510869e+00, ...,\n",
       "        -4.69711248e-02, -3.39395614e-03, -7.58570495e-17],\n",
       "       ...,\n",
       "       [ 5.01994326e-01,  3.38188066e-01, -1.62567715e+01, ...,\n",
       "         2.51138342e-02,  2.78921216e-02,  1.14994409e-17],\n",
       "       [ 4.71899665e-01,  3.12768450e-01, -1.62775561e+01, ...,\n",
       "         2.43849140e-02,  2.65606308e-02,  1.14430892e-17],\n",
       "       [ 4.42299979e-01,  2.87223222e-01, -1.62976321e+01, ...,\n",
       "         2.37557255e-02,  2.54026535e-02,  1.13854924e-17]])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = PCA().fit_transform(data.ns_displacements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "8e5988e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.indexes.datetimes.DatetimeIndex"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data.dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73174a6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
