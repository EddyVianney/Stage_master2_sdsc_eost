{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa0c478f",
   "metadata": {},
   "source": [
    "# Auteur : PAMBOU MOUBOGHA Eddy Vianney\n",
    "# Etudes : Master Sciences des Donnees et Systèmes Complexes\n",
    "# Ecole : Université de Strasbourg\n",
    "# Sujet :  Detection de glissements de terrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b2219c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from datetime import date, datetime\n",
    "import datetime\n",
    "from tslearn.barycenters import dtw_barycenter_averaging\n",
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA, FastICA\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from scipy.spatial.distance import euclidean\n",
    "from sklearn.feature_selection import f_regression\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from sklearn.manifold import TSNE\n",
    "from DBCV import DBCV\n",
    "import hdbscan\n",
    "import csv\n",
    "import multiprocessing\n",
    "import itertools\n",
    "import operator\n",
    "import math\n",
    "from dtaidistance import dtw\n",
    "import seaborn as sns\n",
    "import elevation\n",
    "import json\n",
    "plt.style.use('fivethirtyeight')\n",
    "from osgeo import gdal \n",
    "from subprocess import Popen\n",
    "import simplekml\n",
    "import copy\n",
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378b759e",
   "metadata": {},
   "source": [
    "# Attributs des fichiers MM_TIO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19355f89",
   "metadata": {},
   "source": [
    "* Lat : latitude (degré)\n",
    "* Lon : longitude (degré)\n",
    "* Vel : vitesse (mètre/jour)\n",
    "* Topo : altitude d'un point (mètre)\n",
    "* TS : serie temporelles de déplacement (mètre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "212656fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = './donnees'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f392345",
   "metadata": {},
   "source": [
    "# Paramètres des géométries ascendante et descendante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d07c2e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# angle d'incidence du satellite en géométrie ascendante (angle entre la vertical et la direction du  satellite)\n",
    "theta_asc = 0\n",
    "# angle d'incidence du satelitte en géométrie descendante (angle entre la vertical et la direction du satellite)\n",
    "theta_desc = 0\n",
    "# difference angulaire des orbites des géométries ascendante et descendante\n",
    "delta_alpha =  0\n",
    "# déplacement le long de la ligne de visée en géométrie ascendante\n",
    "d_los_asc = 0\n",
    "# déplacement le long de la ligne de visée en géométrie descendante\n",
    "d_los_dsc = 0\n",
    "# # déplacement vertical\n",
    "d_up = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54df763e",
   "metadata": {},
   "source": [
    "# Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15a1c987",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_slope_value(file, lat, lon, ref='wgs84'):\n",
    "    return float(os.popen('gdallocationinfo -valonly -%s %s %f %f' % (ref, file, lat, lon)).read())\n",
    "    \n",
    "def plot_raster(raster, cmap='gray'):\n",
    "    values = raster.GetRasterBand(1).ReadAsArray()\n",
    "    plt.figure()\n",
    "    plt.imshow(values, cmap = cmap)\n",
    "    plt.colorbar()\n",
    "    plt.axis\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6d7e54e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.899076461792\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_slope_value(ref, file, lat, lon):\n",
    "        val = os.popen('gdallocationinfo -valonly -%s %s %f %f' % (ref, file, lat, lon)).read()\n",
    "        if len(val) == 0:\n",
    "            raise ValueError('La pente est non valide !')\n",
    "        print(val)\n",
    "        return  float(val)\n",
    "    \n",
    "ref = 'wgs84'\n",
    "file = 'rasters/slope_map.tif'\n",
    "lat = 6.662360\n",
    "lon = 44.394169\n",
    "\n",
    "val = os.popen('gdallocationinfo -valonly -%s %s %f %f' % (ref, file, lat, lon)).read()\n",
    "print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded57e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "azimuths = []\n",
    "for lat, lon in zip(latitudes, longitudes):\n",
    "    azimuths.append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef431d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dem = gdal.Open('rasters/31TGK_copernicus_dem.tif')\n",
    "slope_map = gdal.DEMProcessing('rasters/slope_map.tif', dem, 'slope', computeEdges = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0623a569",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_json():\n",
    "    for i, (dirpath, dirnames, filenames) in enumerate(os.walk('./secteurs')):\n",
    "        file_path = os.path.join(dirpath, filename)\n",
    "        file = json.load(open(file_path))\n",
    "        nb_lines   = p['local']['nb_lines']\n",
    "        nb_columns = p['local']['nb_columns']\n",
    "        for nl in range(nb_lines):\n",
    "            for nc in range(nb_columns):\n",
    "                latitude  = file['data'][nl][nc][0]\n",
    "                longitude = file['data'][nl][nc][1]\n",
    "                elevation = file['data'][nl][nc][2]\n",
    "                velocity  = file['data'][nl][nc][3]\n",
    "                quality   = file['data'][nl][nc][4]\n",
    "                serie     = file['data'][nl][nc][5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "708aa22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_date(str):\n",
    "    str_strp = str.strip()\n",
    "    year, month, day = int(str_strp[0:4]), int(str_strp[4:6]), int(str_strp[6:8])\n",
    "    return date(year, month, day)\n",
    "\n",
    "def load_data(filename):\n",
    "\n",
    "    # numéro de la ligne ou commence les données\n",
    "    num_start = 44\n",
    "    # numéro de la ligne ou se trouve la liste des dates\n",
    "    num_list_dates = 40\n",
    "    # attributs présent dans les données\n",
    "    columns = ['id', 'Lat','Lon', 'Topo', 'Vel', 'Coer',' CosN', 'CosE', 'CosU']\n",
    "    # dictionnaire stockant les données\n",
    "    data = {column: [] for column in columns}\n",
    "    # liste des dates \n",
    "    indexes = []\n",
    "    # series temporelles\n",
    "    series = []\n",
    "    # liste de dataframes\n",
    "    df_series = []\n",
    "\n",
    "    with open(DATA_PATH + '/' + filename) as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=\",\")\n",
    "        line_count = 1 \n",
    "        for row in csv_reader:\n",
    "            if line_count == num_list_dates:\n",
    "                indexes = [row[0].split(' ')[1]] + row[1:]\n",
    "            if line_count >= num_start:\n",
    "                # extraction des premiers attributs\n",
    "                for i in range(len(columns)):\n",
    "                    data[columns[i]].append(row[i])\n",
    "                # extraction de l'attribut TS(série temporelle)\n",
    "                series.append([float(v) for v in row[len(columns):]])\n",
    "            line_count  += 1\n",
    "        if len(indexes) != len(series[0]):\n",
    "            print('Erreur : Les indexes et les valeurs ne correspondent pas')\n",
    "        # convertir les index en date\n",
    "        indexes = [d.strip()[0:8] for d in indexes]\n",
    "        # créer une liste de dataframes, chacun contenant une série temporelle\n",
    "        for serie in series:\n",
    "            tmp_serie = pd.DataFrame({'displacement': pd.Series(serie, index=pd.DatetimeIndex(indexes))})\n",
    "            tmp_serie.sort_index(inplace=True)\n",
    "            df_series.append(tmp_serie)\n",
    "        # creer un dataframe pour les autres attributs\n",
    "        df = pd.DataFrame(data)\n",
    "        for column in df.columns:\n",
    "            df[column] = pd.to_numeric(df[column], errors='coerce')\n",
    "        df.set_index('id')\n",
    "        \n",
    "    return df, df_series\n",
    "\n",
    "\n",
    "def get_ids(df, column='id'):\n",
    "    \n",
    "    return df[column].tolist()\n",
    "        \n",
    "def impute(series, ids, pc=0.4):\n",
    "    \n",
    "    result = []\n",
    "    ids_to_not_keep = []\n",
    "    n_samples = len(series[0])\n",
    "    \n",
    "    for i in range(len(series)):\n",
    "        if 100*(series[i].isnull().sum().sum()/n_samples) < pc:\n",
    "            result.append(series[i].interpolate(limit_direction='both', inplace=False))\n",
    "        else:\n",
    "            ids_to_not_keep.append(ids[i])\n",
    "            \n",
    "    return result, ids_to_not_keep\n",
    "\n",
    "def imputed(serie, index=None):\n",
    "    \n",
    "    # dataframe resultat\n",
    "    result = None\n",
    "    \n",
    "    # liste des identifiants des valeurs manquantes\n",
    "    rows_with_nan = [index for index, row in serie.iterrows() if row.isnull().any()]\n",
    "    \n",
    "    if index is None:\n",
    "        result = serie.dropna(inplace=False)\n",
    "        \n",
    "    if isinstance(index, list):\n",
    "        pass\n",
    "        \n",
    "    return result, rows_with_nan\n",
    "        \n",
    "\n",
    "# imputer les données\n",
    "def impute_(ns_series, ew_series, pc=0.4):\n",
    "\n",
    "    # nombre de mésures\n",
    "    n = len(ns_series[0])\n",
    "    \n",
    "    # recupérer les series de la composante Nord-Sud qui ont un pourcentage de valeurs manquuantes inferieur à 50%\n",
    "    ns_booleans  = [True if (ns_series[i].isnull().sum().sum()/n) < pc else False for i in range(len(ns_series))]\n",
    "    # recupérer les series de la composante Est-Ouest qui ont un pourcentage de valeurs manquuantes inferieur à 50%\n",
    "    ew_booleans  = [True if (ew_series[i].isnull().sum().sum()/n) < pc else False for i in range(len(ew_series))]\n",
    "    # Conserver uniquement les points où la norme du vecteur vitesse est calculable\n",
    "    booleans     = [True if i and j else False for i, j in zip(ns_booleans, ew_booleans)]\n",
    "    \n",
    "    # suppression des séries temporelles qui ont un pourcentage de valeurs manquantes supérieure ou égale à 0.5\n",
    "    ns_series_c = [d for d in itertools.compress(ns_series, booleans)]\n",
    "    ew_series_c = [d for d in itertools.compress(ew_series, booleans)]\n",
    "    \n",
    "    # interpoler les valeurs manquantes des les séries restantes\n",
    "    for i in range(len(ns_series_c)):\n",
    "        if ns_series_c[i].isnull().sum().sum() > 0:\n",
    "            ns_series_c[i].interpolate(limit_direction='both', inplace=True)\n",
    "        if ew_series_c[i].isnull().sum().sum() > 0:\n",
    "            ew_series_c[i].interpolate(limit_direction='both', inplace=True)\n",
    "            \n",
    "    return ns_series_c[0].index, ew_series_c, ns_series_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "id": "78691261",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSerie():\n",
    "    \n",
    "    def __init__(self, id_, lat, lon, topo, serie, vlm):\n",
    "        self.id   = id_\n",
    "        self.lat  = lat\n",
    "        self.lon  = lon\n",
    "        self.topo = topo\n",
    "        self.serie = self.interpolate(serie) if self.has_null_values(serie) else serie\n",
    "        self.vlm   = vlm\n",
    "        self.selected = False\n",
    "    \n",
    "    def has_null_values(self, serie):\n",
    "        return True if serie.isnull().sum().sum() > 0 else False\n",
    "    \n",
    "    def compute_null_val_percentage(self):\n",
    "        return 100 *(1.0 * self.serie.isnull().sum().sum() / len(self.serie)  )\n",
    "        \n",
    "    def interpolate(self, serie):\n",
    "        return serie.interpolate(limit_direction='both', inplace=False)\n",
    "        \n",
    "    def set_selected(self):\n",
    "        self.selected = !self.selected\n",
    "    \n",
    "    def compute_pearson_coef(self):\n",
    "        return stats.pearsonr(np.squeeze(self.serie.values), get_days(self.serie.index))\n",
    "        \n",
    "    def get_linear_reg_pval(self, alpha):\n",
    "        \n",
    "        # transformer les index en durée pour pouvoire effectuer une regression linéaire\n",
    "        X = np.array([abs((self.serie.index[0] - self.serie.index[n]).days) for n in range(len(self.serie.index))]).reshape(-1,1)\n",
    "        \n",
    "        # extraire la cible\n",
    "        y = StandardScaler().fit_transform(self.serie)\n",
    "        \n",
    "        # calculer la p-value de la regression lineaire\n",
    "        _, pval = f_regression(X,y.ravel())\n",
    "        \n",
    "        return  pval\n",
    "    \n",
    "    def set_linear_reg_pvalue(pvalue):\n",
    "        self.linear_reg_pvalue = pvalue\n",
    "        \n",
    "    def set_slope(slope):\n",
    "        self.slope = slope\n",
    "        \n",
    "    def get_slope_value(self, ref, file):\n",
    "        val = os.popen('gdallocationinfo -valonly -%s %s %f %f' % (ref, file, self.lat, self.lon)).read()\n",
    "        if len(val) == 0:\n",
    "            raise ValueError('La pente est non valide !')\n",
    "        return  float(val)\n",
    "            \n",
    "    def is_to_select(self, file, ref, min_slope, alpha, sigma, ampl, pc):\n",
    "        slope   = self.get_slope_value(ref, file)\n",
    "        p_value = self.get_linear_reg_pval(alpha)\n",
    "        vlm = self.vlm\n",
    "        # filtrage des series avec peu de valeurs\n",
    "        if self.compute_nul_val_percentage > pc:\n",
    "            return False\n",
    "        # filtrage des regressions non significatives\n",
    "        if p_value > alpha:\n",
    "            return False\n",
    "        # filtrage des vitesses faibles\n",
    "        if abs(vlm) < ampl* sigma:\n",
    "            return False\n",
    "         # filtrage des pentes faibles\n",
    "        if slope < min_slope:\n",
    "            return False\n",
    "        # sauvegarder l'état du pixel\n",
    "        self.set_selected()\n",
    "        return True\n",
    "\n",
    "    def set_linear_reg_pvalue(self, pvalue):\n",
    "        self.linear_reg_pvalue = pvalue\n",
    "        \n",
    "    def load(self, displacement):\n",
    "        return serie.dropna(inplace=False)\n",
    "    \n",
    "    def has_enough_values(self, pc):\n",
    "        return True if 100*(self.serie.displacement.isnull().sum().sum()/len(self.serie)) < pc else False\n",
    "        \n",
    "    def normalize(self):\n",
    "        return StandardScaler().fit_transform(self.serie)\n",
    "        \n",
    "    def compute_inst_vel(self):\n",
    "        vels =  []\n",
    "        for i in range(1, len(self.serie)-1):\n",
    "            duration = (self.serie.index[i+1] - self.serie.index[i-1]).days\n",
    "            displacement = self.serie.iloc[i+1].values[0] - self.serie.iloc[i-1].values[0]\n",
    "            vels.append(displacement / duration)\n",
    "        # supprimer le premier et le dernier index (formule non applicable)\n",
    "        return pd.DataFrame(vels, index=self.serie.index[1:-1], columns=['vel'])\n",
    "    \n",
    "    # la copie renvoie bien un nouvel objet, il n'y a pas d'effets de bord\n",
    "    def smooth(self, ampl=2):\n",
    "        serie = self.serie.copy()\n",
    "        sigma = math.sqrt(serie.var())\n",
    "        for i in range(len(serie)):\n",
    "            if abs(serie.iloc[i].displacement) > ampl*sigma:\n",
    "                # remplacer les anomalies par des valeurs manquantes\n",
    "                serie.iloc[i, serie.columns.get_loc('displacement')]= np.nan\n",
    "        return self.deepcopy(serie.interpolate(limit_direction='both', inplace=False))\n",
    "    \n",
    "    def detect_non_moving_serie(self, alpha):\n",
    "        \n",
    "        X, y = self.prepare()\n",
    "        # calculer la p-value de la regression lineaire\n",
    "        _, pval = f_regression(X,y.ravel())\n",
    "        self.set_linear_reg_pvalue(pval)\n",
    "        \n",
    "        # eliminer le bruit si la regression n'est pas significative (bourrage de zeros)\n",
    "        if pval > alpha:\n",
    "            #return self.clone(pd.DataFrame(0.0, index=self.serie.index, columns=self.serie.columns))\n",
    "            return self.deepcopy(pd.DataFrame(0.0, index=self.serie.index, columns=self.serie.columns))  \n",
    "        else:\n",
    "            #return self.clone(self.serie.copy())\n",
    "            return self.deepcopy(self.serie)\n",
    "    \n",
    "    def transform(self, alpha):\n",
    "        return self.smooth().detect_non_moving_serie(alpha)\n",
    "        \n",
    "    def deepcopy(self, serie):\n",
    "        clone = copy.deepcopy(self)\n",
    "        clone.set_serie(serie)\n",
    "        return clone\n",
    "    \n",
    "    def set_serie(self,serie):\n",
    "        self.serie = serie\n",
    "        \n",
    "    def compute_adfuller(self, ts):\n",
    "        adf_result = adfuller(ts)\n",
    "        adf_output = pd.Series(adf_result[0:4],index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n",
    "        for key, value in adf_result[4].items():\n",
    "            adf_output['Critical Value (%s)'%(key)] = value\n",
    "        return adf_output[1]\n",
    "        \n",
    "    # Il y a un souci avec cette fonction\n",
    "    def clone(self, serie):\n",
    "        return TimeSerie(self.id, self.lat, self.lon, self.topo, serie)\n",
    "    \n",
    "    def get_days(dates):\n",
    "        days = []\n",
    "        for i in range(len(dates)):\n",
    "            days.append(abs((dates[0] - dates[i]).days ))\n",
    "        return days\n",
    "\n",
    "    def get_Xy(self, serie):\n",
    "        X = get_days(serie.index)\n",
    "        y = StandardScaler().fit_transform(serie)\n",
    "        return np.array(X).reshape(-1,1), y\n",
    "    \n",
    "    def prepare(self):\n",
    "        # transformer les index en durée pour pouvoire effectuer une regression linéaire\n",
    "        X = np.array([abs((self.serie.index[0] - self.serie.index[n]).days) for n in range(len(self.serie.index))]).reshape(-1,1)\n",
    "        # extraire la cible\n",
    "        y = StandardScaler().fit_transform(self.serie)\n",
    "        return X, y\n",
    "    \n",
    "    # that functions gives approximately the same result when use sklearn linear regression\n",
    "    # x and y and numpy array\n",
    "    def compute_slope(self, x, y):\n",
    "        return np.cov(x.T, y.T)[0][1] / np.var(x)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "id": "54fb1b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_slope(x, y):\n",
    "    return  np.cov(x,y)[0][1] / np.var(x)\n",
    "\n",
    "def get_days(dates):\n",
    "    days = []\n",
    "    for i in range(len(dates)):\n",
    "        days.append(abs((dates[0] - dates[i]).days ))\n",
    "    return days\n",
    "\n",
    "serie = df_ew_ts[0].interpolate(limit_direction='both', inplace=False)\n",
    "X = np.array([abs((serie.index[0] - serie.index[n]).days) for n in range(len(serie.index))]).reshape(-1,1)\n",
    "        \n",
    "# extraire la cible\n",
    "y = StandardScaler().fit_transform(serie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "id": "69341252",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataFromImageCorrelation():\n",
    "    \n",
    "    def __init__(self, raster_folder_name, dem_filename, geo, ns, ew, pc=0.4, alpha=0.05, ref='wgs84'):\n",
    "        self.ns, self.ew  = self.load(geo, ns, ew, pc)\n",
    "        self.compute_slope_map(raster_folder_name, dem_filename)\n",
    "        self.n_cores = multiprocessing.cpu_count()\n",
    "        self.non_filtered_ids = []\n",
    "        self.pc = pc\n",
    "        self.ref = ref\n",
    "        self.alpha = alpha\n",
    "        self.vlm_std = None\n",
    "        self.magnitudes = None\n",
    "        self.slope_map_path = None\n",
    "        \n",
    "    def compute_vlm_std(self):\n",
    "        return np.std(np.array([math.sqrt(ns.vlm*ns.vlm + ew.vlm*ew.vlm) for (ns, ew) in zip(Data.ns, Data.ew)]))\n",
    "    \n",
    "    def set_slope_map_path(self, slope_map_path):\n",
    "        self.slope_map_path = slope_map_path\n",
    "        \n",
    "    def select_pixel(self, n, min_slope, sigma, ampl):\n",
    "        return (self.ns[n].is_to_select(self.slope_map_path, self.ref, min_slope, self.alpha, sigma, ampl, self.pc) or \n",
    "                self.ew[n].is_to_select(self.slope_map_path, self.ref, min_slope, self.alpha, sigma, ampl, self.pc))\n",
    "    \n",
    "    def load_raster(self, raster_folder_name, raster_filename):\n",
    "        return gdal.Open(raster_folder_name + '/' +  raster_filename)\n",
    "        \n",
    "    def compute_slope_map(self, raster_folder_name, dem_name):\n",
    "        dem = None\n",
    "        slope_map = None\n",
    "        slope_map_name = dem_name.split('.')[0] + '_' + 'slope_map.tif'\n",
    "        slope_map_path = raster_folder_name + '/' + slope_map_name\n",
    "        if not os.path.isfile(slope_map_path):\n",
    "            dem = gdal.Open(raster_folder_name + '/'+ dem_name)\n",
    "            slope_map = gdal.DEMProcessing(slope_map_path, dem, 'slope', computeEdges = True)\n",
    "        self.set_slope_map_path(slope_map_path)\n",
    "    \n",
    "    def empty_non_filtered_ids(self):\n",
    "        self.non_filtered_ids.clear()\n",
    "        \n",
    "    def add_non_filtered_ids(self, n):\n",
    "        self.non_filtered_ids.append(n)\n",
    "    \n",
    "    def load(self, geo, ns, ew, pc):\n",
    "        ns_r, ew_r = [], []\n",
    "        for i in range(len(ns)):\n",
    "            ns_ts = ns[i].interpolate(limit_direction='both', inplace=False)\n",
    "            ew_ts = ew[i].interpolate(limit_direction='both', inplace=False)   \n",
    "            ns_r.append(TimeSerie(geo.iloc[i].id, geo.iloc[i].Lat, geo.iloc[i].Lon, geo.iloc[i].Topo, ns_ts, geo.iloc[i].Vel_ns))\n",
    "            ew_r.append(TimeSerie(geo.iloc[i].id, geo.iloc[i].Lat, geo.iloc[i].Lon, geo.iloc[i].Topo, ew_ts, geo.iloc[i].Vel_ew))\n",
    "        return ns_r, ew_r\n",
    "    \n",
    "    def compute_vel(self, ns_component, ew_component):\n",
    "        return math.sqrt(ns_component * ns_component + ew_component * ew_component)\n",
    "    \n",
    "    def compute_magnitude(self, boolean, min_slope=None):\n",
    "        magnitudes = []\n",
    "        ns_vel_ts = None\n",
    "        ew_vel_ts = None\n",
    "        if boolean:\n",
    "            ns_vel_ts = self.compute_inst_vels(self.ns)\n",
    "            ew_vel_ts = self.compute_inst_vels(self.ew)\n",
    "        else:\n",
    "            ns, ew    = self.transform(min_slope)\n",
    "            ns_vel_ts = self.compute_inst_vels(ns)\n",
    "            ew_vel_ts = self.compute_inst_vels(ew)\n",
    "        for i in range(len(ns_vel_ts)):\n",
    "            vels = []\n",
    "            column = ns_vel_ts[0].columns[0]\n",
    "            for ns_component, ew_component in zip(ns_vel_ts[i][column], ew_vel_ts[i][column]):\n",
    "                vels.append(self.compute_vel(ns_component, ew_component))\n",
    "            df = pd.DataFrame(vels, index=ns_vel_ts[0].index, columns=['magnitude'])\n",
    "            magnitudes.append(df)\n",
    "        return magnitudes\n",
    "    \n",
    "    def compute_inst_vel(self, ts):\n",
    "        vels =  []\n",
    "        for i in range(1, len(ts.serie)-1):\n",
    "            duration = (ts.serie.index[i+1] - ts.serie.index[i-1]).days\n",
    "            displacement = ts.serie.iloc[i+1].values[0] - ts.serie.iloc[i-1].values[0]\n",
    "            vels.append(displacement / duration)\n",
    "        return pd.DataFrame(vels, index=ts.serie.index[1:-1], columns=['vel'])\n",
    "    \n",
    "    def compute_inst_vels(self, ts):\n",
    "        with multiprocessing.Pool(self.n_cores) as p:\n",
    "            results = p.map(self.compute_inst_vel, ts)\n",
    "            return results\n",
    "        \n",
    "    def transform(self, min_slope):\n",
    "        ns, ew = [], []\n",
    "        self.empty_non_filtered_ids()\n",
    "        vlms_std = self.compute_vlm_std()\n",
    "        for n in range(len(self.ns)):\n",
    "            if self.select_pixel(n, min_slope, vlms_std, self.pc):\n",
    "                ns.append(self.ns[n].smooth().detect_non_moving_serie(self.alpha))\n",
    "                ew.append(self.ew[n].smooth().detect_non_moving_serie(self.alpha))\n",
    "                self.add_non_filtered_ids(n)\n",
    "        return ns, ew\n",
    "    \n",
    "    # renvoyer les données pour effectuer le clustering\n",
    "    # si on n'interesse qu'à la forme, il faut normaliser les données\n",
    "    def prepare(self, min_slope):\n",
    "        self.set_magnitudes(self.compute_magnitude(False, min_slope))\n",
    "    \n",
    "    # for the moment, we only normalize velocity time series\n",
    "    # use this method to use euclidian distance\n",
    "    def normalize(self):\n",
    "        return np.array([StandardScaler().fit_transform(df).reshape(len(df)) for df in self.magnitudes])\n",
    "    \n",
    "    def set_magnitudes(self, magnitudes):\n",
    "        self.magnitudes = magnitudes\n",
    "        \n",
    "    # use this matrix to use DTW with HDBSCAN\n",
    "    def compute_similarity_matrix(self):\n",
    "        size = len(self.magnitudes)\n",
    "        distances_matrix = np.zeros(shape=(len(self.magnitudes), len(self.magnitudes)))\n",
    "        for n in range(size):\n",
    "            for m in range(n , size):\n",
    "                s1 = StandardScaler().fit_transform(self.magnitudes[n])\n",
    "                s2 = StandardScaler().fit_transform(self.magnitudes[m])\n",
    "                dist = dtw.distance(s1, s2)\n",
    "                distances_matrix[n, m] = dist\n",
    "                distances_matrix[m, m] = dist\n",
    "        return distances_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23235d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Visualization():\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def plot_displacement(self, ts):\n",
    "        \n",
    "        days = get_days(ns_ts.index)\n",
    "        fig, ax = plt.subplots(figsize=(15,5))\n",
    "    \n",
    "        ax[0].plot(ns_ts['displacement'], color='blue', label='NS displacement (m)', marker='o', linewidth=2)\n",
    "        ax[1].plot(ew_ts['displacement'], color='orange', label='EW displacement (m)', marker='o', linewidth=2)\n",
    "        \n",
    "        plt.legend()\n",
    "        plt.legend(loc='best')\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_displacement_vel(self, ns_ts, ew_ts):\n",
    "        \n",
    "        days = get_days(ns_ts[0].index)\n",
    "\n",
    "        fig, ax_left = plt.subplots(figsize=(15,5))\n",
    "        ax_right = ax_left.twinx()\n",
    "\n",
    "        p1, = ax_left.plot(days, df_series[4]['displacement'], color='blue', label='NS displacement (m)')\n",
    "        p2, = ax_right.plot(days, df_series[4]['displacement'], color='orange', label='EW displacement(m)')\n",
    "\n",
    "        ax_left.set_xlabel(\"number of days since the first measure\")\n",
    "        ax_left.set_ylabel(\"displacement\")\n",
    "        ax_right.set_ylabel(\"velocity\")\n",
    "\n",
    "        lns = [p1, p2]\n",
    "\n",
    "        ax_left.legend(handles=lns, loc='best')\n",
    "        fig.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    def plot_disp_ns_ew(self, ns_ts, ew_ts):\n",
    "\n",
    "        fig, ax = plt.subplots(2, 1, figsize=(15,10))\n",
    "\n",
    "        ax[0].plot(ns_ts, color='blue', label='displacement (m)', marker='o', linewidth=2)\n",
    "        ax[0].set_title('NS cumulative displacement')\n",
    "        ax[0].set_xlabel('time')\n",
    "        ax[0].set_ylabel('displacement')\n",
    "        ax[0].legend()\n",
    "\n",
    "        ax[1].plot(ew_ts, color='orange', label='displacement (m)', marker='o', linewidth=2)\n",
    "        ax[1].set_title('EW cumulative displacement')\n",
    "        ax[1].set_xlabel('time')\n",
    "        ax[1].set_ylabel('displacement')\n",
    "        ax[1].legend()\n",
    "        \n",
    "        fig.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "    def plot_disp_vel(self, ns_ts, ew_ts, vels):\n",
    "\n",
    "        fig, ax = plt.subplots(3, 1, figsize=(15,10))\n",
    "\n",
    "        ax[0].plot(ns_ts, color='blue', label='displacement (m)', marker='o', linewidth=2)\n",
    "        ax[0].set_title('NS cumulative displacement')\n",
    "        ax[0].set_xlabel('time')\n",
    "        ax[0].set_ylabel('displacement')\n",
    "        ax[0].legend()\n",
    "\n",
    "        ax[1].plot(ew_ts, color='orange', label='displacement (m)', marker='o', linewidth=2)\n",
    "        ax[1].set_title('EW cumulative displacement')\n",
    "        ax[1].set_xlabel('time')\n",
    "        ax[1].set_ylabel('displacement')\n",
    "        ax[1].legend()\n",
    "\n",
    "        ax[2].plot(vels, color='green', label='vSyntaxError: invalid syntaxelocity (m/day)', marker='o', linewidth=2)\n",
    "        ax[2].set_title('Velocity magnitude')\n",
    "        ax[2].set_xlabel('time')\n",
    "        ax[2].set_ylabel('velocity')\n",
    "        ax[2].legend()\n",
    "\n",
    "        fig.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    def plot_series(series, num_rows=4, num_cols=5, colormap='tab20'):\n",
    "        \n",
    "        plot_kwds = {'alpha' : 0.25, 's' : 10, 'linewidths':0}\n",
    "        fig, axs = plt.subplots(num_rows, num_cols, figsize=(25,25))\n",
    "        cmap = plt.get_cmap(colormap)\n",
    "        colors = [cmap(i) for i in np.linspace(0,1,num_rows*num_cols)]\n",
    "\n",
    "        for num_row in range(num_rows):\n",
    "            for num_col in range(num_cols):\n",
    "                if num_row*num_cols + num_col < len(series):\n",
    "                    axs[num_row, num_col].plot(series[num_row*num_cols + num_col], color=colors[num_row*num_cols + num_col], marker='o', markerfacecolor='white')\n",
    "                    #axs[num_row, num_col].set_title('serie: %s'%(self.names[num_row*num_cols + num_col].split('.')[0]))\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "c8545214",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-321-0508635d9d2a>, line 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-321-0508635d9d2a>\"\u001b[0;36m, line \u001b[0;32m12\u001b[0m\n\u001b[0;31m    if start <= date <= end\u001b[0m\n\u001b[0m                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class Utility():\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def get_season(self, date):\n",
    "        \n",
    "        for season in seasons:\n",
    "            name  = season[0]\n",
    "            start = season[1].replace(year=date.year)\n",
    "            end   = season[2].replace(year=date.year)\n",
    "            if start <= date <= end\n",
    "                return name\n",
    "            \n",
    "    def count_samples_per_season(self, dates):\n",
    "    \n",
    "        samples_per_season = {'winter': 0, 'spring': 0, 'summer': 0, 'autumn': 0}\n",
    "        for date in dates:\n",
    "            samples_per_season[get_season(date)] += 1\n",
    "\n",
    "        return samples_per_season\n",
    "    \n",
    "    def get_days(self, dates):\n",
    "        days = []\n",
    "        for i in range(len(dates)):\n",
    "            days.append(abs((dates[0] - dates[i]).days ))\n",
    "        return days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "id": "6a986683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chargement de la composante  Est-Ouest du mouvement du sol\n",
    "df_ew, df_ew_ts = load_data('MM_TIO_EW_31TGK_20151227_to_20200906.csv')\n",
    "# chargement de la composante Nord-Sud du mouvement du sol\n",
    "df_ns, df_ns_ts = load_data('MM_TIO_NS_31TGK_20151227_to_20200906.csv')\n",
    "# fusionner les fichiers sur la topo et les vitesses (pour éviter la redondance de l'information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "id": "16cd260c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ew.rename(columns={'Vel': 'Vel_ew'}, inplace=True)\n",
    "df_ns.rename(columns={'Vel': 'Vel_ns'}, inplace=True)\n",
    "geo = pd.concat([df_ew[['id', 'Lat','Lon','Topo','Vel_ew']], df_ns[['Vel_ns']]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "id": "cd7fd691",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 30\n",
    "RASTER_FOLDER_PATH = 'rasters'\n",
    "DEM_FILENAME =  '31TGK_copernicus_dem.tif'\n",
    "Data = DataFromImageCorrelation(RASTER_FOLDER_PATH, DEM_FILENAME, geo.head(n), df_ns_ts[:n], df_ew_ts[:n])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff15e6c",
   "metadata": {},
   "source": [
    "# Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "775eb843",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-3:\n",
      "Process ForkPoolWorker-1:\n",
      "Process ForkPoolWorker-2:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/eost-user/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-4:\n",
      "  File \"/home/eost-user/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/eost-user/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/eost-user/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/eost-user/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 48, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"/home/eost-user/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-10-a95ae9c86253>\", line 76, in compute_inst_vel\n",
      "    duration = (ts.serie.index[i+1] - ts.serie.index[i-1]).days\n",
      "  File \"/home/eost-user/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/eost-user/miniconda3/lib/python3.9/site-packages/pandas/core/indexes/extension.py\", line 248, in __getitem__\n",
      "    deprecate_ndim_indexing(result)\n",
      "  File \"/home/eost-user/miniconda3/lib/python3.9/site-packages/pandas/core/indexers.py\", line 322, in deprecate_ndim_indexing\n",
      "    if np.ndim(result) > 1:\n",
      "  File \"/home/eost-user/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 48, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"<__array_function__ internals>\", line 5, in ndim\n",
      "  File \"/home/eost-user/miniconda3/lib/python3.9/site-packages/numpy/core/fromnumeric.py\", line 3138, in ndim\n",
      "    try:\n",
      "KeyboardInterrupt\n",
      "  File \"/home/eost-user/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/eost-user/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/eost-user/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"<ipython-input-10-a95ae9c86253>\", line 77, in compute_inst_vel\n",
      "    displacement = ts.serie.iloc[i+1].values[0] - ts.serie.iloc[i-1].values[0]\n",
      "  File \"/home/eost-user/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/eost-user/miniconda3/lib/python3.9/site-packages/pandas/core/indexing.py\", line 895, in __getitem__\n",
      "    return self._getitem_axis(maybe_callable, axis=axis)\n",
      "  File \"/home/eost-user/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/eost-user/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 48, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"/home/eost-user/miniconda3/lib/python3.9/site-packages/pandas/core/indexing.py\", line 1503, in _getitem_axis\n",
      "    return self.obj._ixs(key, axis=axis)\n",
      "  File \"<ipython-input-10-a95ae9c86253>\", line 77, in compute_inst_vel\n",
      "    displacement = ts.serie.iloc[i+1].values[0] - ts.serie.iloc[i-1].values[0]\n",
      "  File \"/home/eost-user/miniconda3/lib/python3.9/site-packages/pandas/core/frame.py\", line 2954, in _ixs\n",
      "    name=self.index[i],\n",
      "  File \"/home/eost-user/miniconda3/lib/python3.9/site-packages/pandas/core/indexing.py\", line 895, in __getitem__\n",
      "    return self._getitem_axis(maybe_callable, axis=axis)\n",
      "  File \"/home/eost-user/miniconda3/lib/python3.9/site-packages/pandas/core/indexes/extension.py\", line 248, in __getitem__\n",
      "    deprecate_ndim_indexing(result)\n",
      "  File \"/home/eost-user/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/eost-user/miniconda3/lib/python3.9/site-packages/pandas/core/indexing.py\", line 1503, in _getitem_axis\n",
      "    return self.obj._ixs(key, axis=axis)\n",
      "  File \"/home/eost-user/miniconda3/lib/python3.9/site-packages/pandas/core/indexers.py\", line 322, in deprecate_ndim_indexing\n",
      "    if np.ndim(result) > 1:\n",
      "  File \"/home/eost-user/miniconda3/lib/python3.9/site-packages/pandas/core/frame.py\", line 2951, in _ixs\n",
      "    result = self._constructor_sliced(\n",
      "  File \"/home/eost-user/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 48, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"/home/eost-user/miniconda3/lib/python3.9/site-packages/pandas/core/series.py\", line 364, in __init__\n",
      "    data = sanitize_array(data, index, dtype, copy, raise_cast_failure=True)\n",
      "  File \"<ipython-input-10-a95ae9c86253>\", line 77, in compute_inst_vel\n",
      "    displacement = ts.serie.iloc[i+1].values[0] - ts.serie.iloc[i-1].values[0]\n",
      "  File \"<__array_function__ internals>\", line 5, in ndim\n",
      "  File \"/home/eost-user/miniconda3/lib/python3.9/site-packages/pandas/core/construction.py\", line 445, in sanitize_array\n",
      "    data = extract_array(data, extract_numpy=True)\n",
      "  File \"/home/eost-user/miniconda3/lib/python3.9/site-packages/numpy/core/fromnumeric.py\", line 3138, in ndim\n",
      "    try:\n",
      "KeyboardInterrupt\n",
      "  File \"/home/eost-user/miniconda3/lib/python3.9/site-packages/pandas/core/indexing.py\", line 895, in __getitem__\n",
      "    return self._getitem_axis(maybe_callable, axis=axis)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/eost-user/miniconda3/lib/python3.9/site-packages/pandas/core/indexing.py\", line 1503, in _getitem_axis\n",
      "    return self.obj._ixs(key, axis=axis)\n",
      "  File \"/home/eost-user/miniconda3/lib/python3.9/site-packages/pandas/core/frame.py\", line 2951, in _ixs\n",
      "    result = self._constructor_sliced(\n",
      "  File \"/home/eost-user/miniconda3/lib/python3.9/site-packages/pandas/core/series.py\", line 286, in __init__\n",
      "    dtype = self._validate_dtype(dtype)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/eost-user/miniconda3/lib/python3.9/site-packages/pandas/core/generic.py\", line 340, in _validate_dtype\n",
      "    @final\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-7362ea000b2c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# visualisation des vitesses brutes (calculées à partir des déplacement brutes)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mraw_magnitudes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_magnitude\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mviz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVisualization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mviz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_disp_vel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserie\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mew\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserie\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_magnitudes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-a95ae9c86253>\u001b[0m in \u001b[0;36mcompute_magnitude\u001b[0;34m(self, boolean, min_slope)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mew_vel_ts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mboolean\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mns_vel_ts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_inst_vels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m             \u001b[0mew_vel_ts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_inst_vels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mew\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-a95ae9c86253>\u001b[0m in \u001b[0;36mcompute_inst_vels\u001b[0;34m(self, ts)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcompute_inst_vels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmultiprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_cores\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m             \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_inst_vel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/multiprocessing/pool.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mthat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mreturned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m         '''\n\u001b[0;32m--> 364\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapstar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstarmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    766\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    760\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    572\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 574\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    575\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# visualisation des vitesses brutes (calculées à partir des déplacement brutes)\n",
    "n = 0\n",
    "raw_magnitudes = Data.compute_magnitude(True)\n",
    "viz = Visualization()\n",
    "viz.plot_disp_vel(Data.ns[n].serie, Data.ew[n].serie, raw_magnitudes[n])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05645d88",
   "metadata": {},
   "source": [
    "# Calcul des profils de vitesse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "d0752608",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.prepare(min_slope=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "7a8ef1d9",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-198-0e252e6238f2>, line 47)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-198-0e252e6238f2>\"\u001b[0;36m, line \u001b[0;32m47\u001b[0m\n\u001b[0;31m    plt.show()nb_lines': 50, 'nb_columns': 100,\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#from datetime import datetime\n",
    "Y = 2000\n",
    "seasons  = [('winter', date(Y,  1,  1), date(Y,  3, 20)),\n",
    "           ('spring', date(Y,  3, 21),  date(Y,  6, 20)),\n",
    "           ('summer', date(Y,  6, 21),  date(Y,  9, 22)),\n",
    "           ('autumn', date(Y,  9, 23),  date(Y, 12, 20)),\n",
    "           ('winter', date(Y, 12, 21),  date(Y, 12, 31))]\n",
    "\n",
    "        \n",
    "def plot_pie(labels, values):\n",
    "\n",
    "    fig1, ax1 = plt.subplots(figsize=(5,5))\n",
    "    ax1.pie(values, labels=labels, autopct='%1.1f%%',\n",
    "            shadow=True, startangle=90)\n",
    "    ax1.axis('equal') \n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "def plot_displacement(ns_ts, ew_ts):\n",
    "    \n",
    "    days = get_days(ns_ts.index)\n",
    "    fig, ax = plt.subplots(figsize=(15,5))\n",
    "    \n",
    "    ax.plot(ns_ts['displacement'], color='blue', label='NS displacement (m)', marker='o', linewidth=2)\n",
    "    ax.plot(ew_ts['displacement'], color='orange', label='EW displacement (m)', marker='o', linewidth=2)\n",
    "    \n",
    "    plt.legend(loc='best')\n",
    "    \n",
    "def plot_displacement_velocity(ns_ts, ew_ts):\n",
    "\n",
    "    days = get_days(ns_ts[0].index)\n",
    "\n",
    "    fig, ax_left = plt.subplots(figsize=(15,5))\n",
    "    ax_right = ax_left.twinx()\n",
    "    \n",
    "    p1, = ax_left.plot(days, df_series[4]['displacement'], color='blue', label='NS displacement (m)')\n",
    "    p2, = ax_right.plot(days, df_series[4]['displacement'], color='orange', label='EW displacement(m)')\n",
    "\n",
    "    ax_left.set_xlabel(\"number of days since the first measure\")\n",
    "    ax_left.set_ylabel(\"displacement\")\n",
    "    ax_right.set_ylabel(\"velocity\")\n",
    "\n",
    "    lns = [p1, p2]\n",
    "    \n",
    "    ax_left.legend(handles=lns, loc='best')\n",
    "    fig.tight_layout()\n",
    "    plt.show()nb_lines': 50, 'nb_columns': 100,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbc1d4c",
   "metadata": {},
   "source": [
    "# Clustering par Densité"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332401af",
   "metadata": {},
   "source": [
    "## HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "92a07e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Clustering():\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.result = None\n",
    "        self.n_clusters = None\n",
    "        self.STANDARDSCALER = 0\n",
    "        self.ROBUSTSCALER   = 1\n",
    "    \n",
    "    def select_scaler(serie, option):\n",
    "        if option == 0:\n",
    "            return StandardScaler().fit_transform(serie)\n",
    "        else:\n",
    "            return RobustScaler.fit_transform(serie)\n",
    "        \n",
    "    def normalize(series):\n",
    "        return np.array([StandardScaler().fit_transform(serie).reshape(len(serie)) for serie in series])\n",
    "    \n",
    "    def cluster(self, min_cluster_size, precomputed=False):      \n",
    "        if not precomputed:\n",
    "            self.result = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size, gen_min_span_tree=True).fit(self.data.normalize())\n",
    "        else:\n",
    "            self.result = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size, gen_min_span_tree=True, metric='precomputed').fit(self.data.compute_similarity_matrix())\n",
    "\n",
    "    def get_n_clusters(self):\n",
    "        return 0 if self.result.labels_.max() < 0 else self.result.labels_.max() + 1\n",
    "    \n",
    "    def visualize(self):\n",
    "        projection = TSNE().fit_transform(self.data.normalize())\n",
    "        color_palette = sns.color_palette('Paired', self.result.labels_.max() + 1 )\n",
    "        cluster_colors = [color_palette[x] if x >= 0\n",
    "                  else (0.5, 0.5, 0.5)\n",
    "                  for x in self.result.labels_]\n",
    "        cluster_member_colors = [sns.desaturate(x, p) for x, p in\n",
    "                         zip(cluster_colors, self.result.probabilities_)]\n",
    "        plt.scatter(*projection.T, s=20, c=cluster_member_colors , linewidth=0, alpha=0.25)\n",
    "    plt.show()\n",
    "        \n",
    "    def plot_cluster_distribution(self, colormap='Dark2'):\n",
    "        v_count = dict()\n",
    "        cmap = plt.get_cmap(colormap)\n",
    "        colors = [cmap(i) for i in np.linspace(0, 1, self.result.labels_.max() + 1)]\n",
    "        for label in self.result.labels_:\n",
    "            if label != -1:\n",
    "                if label in v_count.keys():\n",
    "                    v_count[label] += 1\n",
    "                else:\n",
    "                    v_count[label] = 1\n",
    "        pd.Series({k: v for k, v in sorted(v_count.items(), key=lambda item: item[0])}).plot(kind='bar', color=colors)\n",
    "        return v_count\n",
    "    \n",
    "    def get_data_from_class(self, num_label):\n",
    "        data = []\n",
    "        for i in range(len(self.result.labels_)):\n",
    "            if self.result.labels_[i] == num_label:\n",
    "                data.append(self.data[i])\n",
    "        return data\n",
    "    \n",
    "    def plot_cluster_result(self, n_cols=3):\n",
    "        labels = self.result.labels_\n",
    "        n_clusters = labels.max() + 1\n",
    "        n_rows = int(n_clusters / n_cols) if n_clusters % n_cols == 0 else int(math.ceil(n_clusters / n_cols))\n",
    "        fig, axs = plt.subplots(n_rows, n_cols, figsize=(25,15))\n",
    "        for num_cluster in range(n_clusters):\n",
    "            for serie_index in range(len(self.data.magnitudes)):\n",
    "                if labels[serie_index] == num_cluster:\n",
    "                    axs[int(num_cluster / n_cols), num_cluster % n_cols].plot(self.data.magnitudes[serie_index], c='blue', alpha=0.2)\n",
    "            axs[int(num_cluster / n_cols), num_cluster % n_cols].set_title('Cluster %d'%(num_cluster + 1))\n",
    "        fig.tight_layout()\n",
    "        \n",
    "    # créer un fichier csv contenant les champs : id, lat, lon, numero de la classe\n",
    "    def save_result(self):\n",
    "\n",
    "        fieldnames = ['id', 'Lat', 'Lon', 'cluster']\n",
    "        rows = []\n",
    "        \n",
    "        for n in range(len(self.data.ns)):\n",
    "            rows.append({'id': int(self.data.ns[n].id), 'Lat': self.data.ns[n].lat, 'Lon': self.data.ns[n].lon, 'cluster': self.result.labels_[n]})\n",
    "\n",
    "        with open('clustering_result.csv', 'w', encoding='UTF8', newline='') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            writer.writerows(rows)\n",
    "            \n",
    "    def generate_kml_file(self, scale=2):\n",
    "        labels = self.result.labels_\n",
    "        if labels.max() > -1:\n",
    "            icon_links, scales = self.generate_icones(labels)\n",
    "            kml=simplekml.Kml()\n",
    "            fol = kml.newfolder(name=\"HDBSCAN Clustering\")\n",
    "            labels = self.result.labels_\n",
    "            for i, n in enumerate(self.data.non_filtered_ids, 0):\n",
    "                if labels[i] > - 1:\n",
    "                    pnt = fol.newpoint(coords=[(self.data.ns[n].lat, self.data.ns[n].lon)])\n",
    "                    pnt.iconstyle.icon.href =icon_links[labels[i]]\n",
    "                    pnt.style.labelstyle.scale = scales[labels[i]]\n",
    "            kml.save('clustering_result.kml')\n",
    "        else:\n",
    "            print('Hdbscan only found outliers. kml file cannot be generated !')\n",
    "            \n",
    "    def get_pixel_icon(self, m):\n",
    "        labels = self.result.labels_\n",
    "        icon_links, _ = self.generate_icones(labels)\n",
    "        for i, n in enumerate(self.data.non_filtered_ids, 0):\n",
    "            if n == m:\n",
    "                return icon_links[labels[i]]\n",
    "    \n",
    "    #http://tancro.e-central.tv/grandmaster/markers/google-icons/mapfiles-ms-micons.html\n",
    "    def generate_icones(self, labels, scale=2):\n",
    "        BASE = 'http://maps.google.com/mapfiles/ms/micons/'\n",
    "        scales = None\n",
    "        colors = ['blue', 'red', 'yellow', 'green', 'orange', 'purple', 'pink']\n",
    "        icon_links = [BASE + color + '-dot.png' for color in (colors + ['Itblue'])[:]] + [BASE + color + '.png' for color in (colors + ['lightblue'])[:]]\n",
    "        n_clusters = labels.max() + 1\n",
    "        if n_clusters > len(icon_links):\n",
    "            q = int(n_clusters / len(icon_links))\n",
    "            r = n_clusters - q*len(icon_links)\n",
    "            icon_links = (icon_links*q)[:] + icon_links[:r]\n",
    "            scales = [scale*t for t in range(len(icon_links))]\n",
    "        else:\n",
    "            scales = [scale]*n_clusters\n",
    "        return icon_links, scales\n",
    "    \n",
    "    # coefficient de silhouette ?\n",
    "    def validate(self):\n",
    "        return DBCV(self.data.magnitudes, self.result.labels_, dist_function=euclidean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "5b70167d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eost-user/miniconda3/lib/python3.9/site-packages/hdbscan/hdbscan_.py:217: UserWarning: Cannot generate Minimum Spanning Tree; the implemented Prim's does not produce the full minimum spanning tree \n",
      "  warn('Cannot generate Minimum Spanning Tree; '\n"
     ]
    }
   ],
   "source": [
    "import simplekml\n",
    "clustering = Clustering(Data)\n",
    "clustering.cluster(min_cluster_size=30, precomputed=False)\n",
    "clustering.visualize()\n",
    "clustering.generate_kml_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e9ba9e26",
   "metadata": {},
   "outputs": [],
   "source": [
    " def get_pval(serie, alpha=0.05):\n",
    "        \n",
    "        # transformer les index en durée pour pouvoire effectuer une regression linéaire\n",
    "        X = np.array([abs((serie.index[0] - serie.index[n]).days) for n in range(len(serie.index))]).reshape(-1,1)\n",
    "        \n",
    "        # extraire la cible\n",
    "        y = StandardScaler().fit_transform(serie)\n",
    "        \n",
    "        # calculer la p-value de la regression lineaire\n",
    "        _, pval = f_regression(X,y.ravel())\n",
    "        \n",
    "        if pval > alpha:\n",
    "            print('non significative')\n",
    "        else:\n",
    "            print('significative')\n",
    "            \n",
    "def smooth(serie_, ampl=2):\n",
    "    serie = serie_.copy()\n",
    "    sigma = math.sqrt(serie.var())\n",
    "    for i in range(len(serie)):\n",
    "        if abs(serie.iloc[i].displacement) > ampl*sigma:\n",
    "            # remplacer les anomalies par des valeurs manquantes\n",
    "            serie.iloc[i, serie.columns.get_loc('displacement')]= np.nan\n",
    "    return serie.interpolate(limit_direction='both', inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "9834e0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non significative\n"
     ]
    }
   ],
   "source": [
    "#s = pd.DataFrame(0.0, index=Data.ns[n].serie.index, columns=['pp'])\n",
    "get_pval(Data.ns[n].serie)\n",
    "# attention il y a bug quelque part, il faut s'assurer de calculer la regression uniquement après avoir lisser\n",
    "# les pics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "id": "cc633023",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_adfuller(ts):\n",
    "    adf_result = adfuller(ts)\n",
    "    adf_output = pd.Series(adf_result[0:4],index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n",
    "    for key, value in adf_result[4].items():\n",
    "        adf_output['Critical Value (%s)'%(key)] = value\n",
    "    return adf_output[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "240ad472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "serie stationnaire\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "895"
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = compute_adfuller(Data.ns[n].serie)\n",
    "if t < 0.05:\n",
    "    print('serie stationnaire')\n",
    "else:\n",
    "    print('non stationnaire')\n",
    "len(Data.non_filtered_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "39059fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_days(dates):\n",
    "    days = []\n",
    "    for i in range(len(dates)):\n",
    "        days.append(abs((dates[0] - dates[i]).days ))\n",
    "    return np.array(days, dtype='int')\n",
    "from statsmodels.tsa.stattools import kpss\n",
    "\n",
    "def kpss_test(series, **kw):    \n",
    "    statistic, p_value, n_lags, critical_values = kpss(series, **kw)\n",
    "    # Format Output\n",
    "    print(f'KPSS Statistic: {statistic}')\n",
    "    print(f'p-value: {p_value}')\n",
    "    print(f'num lags: {n_lags}')\n",
    "    print('Critial Values:')\n",
    "    for key, value in critical_values.items():\n",
    "        print(f'   {key} : {value}')\n",
    "    print(f'Result: The series is {\"not \" if p_value < 0.05 else \"\"}stationary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1ab7690",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d419f54241f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclustering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "labels = np.array(clustering.result.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "id": "3f6c5d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth(serie_, ampl=2):\n",
    "    serie = serie_.copy()\n",
    "    sigma = math.sqrt(serie.var())\n",
    "    for i in range(len(serie)):\n",
    "        if abs(serie.iloc[i].displacement) > ampl*sigma:\n",
    "            # remplacer les anomalies par des valeurs manquantes\n",
    "            serie.iloc[i, serie.columns.get_loc('displacement')]= np.nan\n",
    "    return serie.interpolate(limit_direction='both', inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "id": "b27d999f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stat=-42.038, p=0.000\n",
      "Probably different distributions\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "\n",
    "data1 = np.squeeze(Data.ew[n].serie.values)\n",
    "data2 = np.full(data1.shape[0], 1.5)\n",
    "stat, p = ttest_ind(np.squeeze(smooth(Data.ew[n].serie).values), data2)\n",
    "print('stat=%.3f, p=%.3f' % (stat, p))\n",
    "if p > 0.05:\n",
    "    print('Probably the same distribution')\n",
    "else:\n",
    "    print('Probably different distributions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8ef3750",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Visualization' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-adc201c50b47>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m36982\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mviz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVisualization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnon_filtered_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mviz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_disp_ns_ew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserie\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mew\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserie\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Visualization' is not defined"
     ]
    }
   ],
   "source": [
    "n = 35773\n",
    "viz = Visualization()\n",
    "index = Data.non_filtered_ids\n",
    "viz.plot_disp_ns_ew(Data.ns[n].serie , Data.ew[n].serie)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73b8c07",
   "metadata": {},
   "source": [
    "# ICA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "5bc1c5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With ICA\n",
    "# S = WX\n",
    "# S_ = fastICA().fit_transform(X)\n",
    "# W : matrice des poids\n",
    "# X : matrice des observations. X.shape = (n_samples, n_sources)\n",
    "# S: les sources\n",
    "# S_ : les sources reconstitués . S_.shape = X.shape\n",
    "\n",
    "# With PCA :\n",
    "# r = PCA().fit_transform(X)\n",
    "# r: les sources reconstutées avec l'ACP\n",
    "\n",
    "# temporal ICA \n",
    "# X = A S\n",
    "# X is the mixed observations (contains the mixed as rows), X.shape = (n_pixel, n_samples)\n",
    "# A.shape = (n_source, n_pixel), les lignes de A sont les axes d'un nouveau repère dans lequel S est projeté\n",
    "# S contains the unknown sources of as rows vectors S.shape = (n_source, n_samples)\n",
    "# on retient que S = WX (unmixing process)\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.decomposition import FastICA\n",
    "\n",
    "def reshape(data):\n",
    "    n_pixels =  len(data)\n",
    "    n_samples = len(data[0])\n",
    "    return  np.array(data).reshape(n_pixels, n_samples)\n",
    "\n",
    "def reshape2(ns):\n",
    "    n_pixels  = len(ns)\n",
    "    n_samples = len(ns[0].serie)\n",
    "    return np.array([s.serie in ns.ns]).reshape(n_pixels, n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "a336d772",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = reshape([s.serie for s in Data.ns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "6d9117e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2990, 3)"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S_ = FastICA(n_components=3).fit_transform(X)\n",
    "S_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a57d42d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-73-d65c0f2494ea>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-73-d65c0f2494ea>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    X*\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# le lignes de la matrice W sont les sources\n",
    "# Pour calculer l'activité d'une source, il faut multiplier X par le vecteur ligne\n",
    "# calcul de l'activité de la première source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d72a4d",
   "metadata": {},
   "source": [
    "# Significativité des régressions linéaires\n",
    "* p-value : probabilité que la pente soit nulle\n",
    "* Hypothèse nulle HO : la pente de la droite de regression est nulle  (vitesse moyenne non significative)\n",
    "* si la p-value est inférieure à 0.05 alors l'hypothèse nulle est rejetée (vitesse moyenne est significative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8cc7e66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from statsmodels.api import add_constant\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5be0170f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_days(dates):\n",
    "    days = []\n",
    "    for i in range(len(dates)):\n",
    "        days.append(abs((dates[0] - dates[i]).days ))\n",
    "    return days\n",
    "\n",
    "def get_xy(serie):\n",
    "    X = get_days(serie.index)\n",
    "    y = StandardScaler().fit_transform(serie)\n",
    "    return np.array(X).reshape(-1,1), y\n",
    "\n",
    "def compute_linear_regression(X,y):\n",
    "    \n",
    "    X_ = add_constant(X)\n",
    "    mod = sm.OLS(y,X_)\n",
    "    fii = mod.fit()\n",
    "    \n",
    "    return fii.params, fii.pvalues\n",
    "\n",
    "def compute_slope(displacement, days):\n",
    "    return  np.cov(displacement, days)[0][1] / np.var(np.array(days))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ab394c",
   "metadata": {},
   "source": [
    "## Test de Dickey-Fuller augmenté (ADF)\n",
    "La série temporelle est considérée comme stationnaire si la valeur p est faible (selon l’hypothèse nulle) et si les valeurs critiques à des intervalles de confiance de 1%, 5%, 10% sont aussi proches que possible des statistiques de l’ADF (Augmented Dickey-Fuller)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6f1541",
   "metadata": {},
   "source": [
    "# Réechantillonage des données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e50929",
   "metadata": {},
   "source": [
    "# Etude de la saisonnalité"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bfa777e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.array([[0, 0, 0], [0, 1, 1], [1, 0, 1], [1, 1, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ea15115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5420e525",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
